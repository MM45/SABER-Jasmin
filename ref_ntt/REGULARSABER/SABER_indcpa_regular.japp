








param int CRYPTO_SECRETKEYBYTES = 2304;
param int CRYPTO_PUBLICKEYBYTES = (3 * 320 + 32);
param int CRYPTO_BYTES = 32;
param int CRYPTO_CIPHERTEXTBYTES = 1088;
param int Saber_type = 2;

param int SABER_K = 3;
param int SABER_MU = 8;
param int SABER_ET = 4;

param int SABER_EQ = 13;
param int SABER_EP = 10;

param int SABER_N = 256;
param int SABER_Q = 8192;
param int SABER_P = 1024;

param int SABER_SEEDBYTES = 32;
param int SABER_NOISESEEDBYTES = 32;
param int SABER_COINBYTES = 32;
param int SABER_KEYBYTES = 32;

param int SABER_HASHBYTES = 32;

param int SABER_POLYBYTES = 416;

param int SABER_POLYVECBYTES = (SABER_K * SABER_POLYBYTES);

param int SABER_POLYVECCOMPRESSEDBYTES = (SABER_K * 320);

param int SABER_CIPHERTEXTBYTES = (SABER_POLYVECCOMPRESSEDBYTES);



param int SABER_SCALEBYTES_KEM = (SABER_ET * SABER_N / 8);

param int SABER_INDCPA_PUBLICKEYBYTES = (SABER_POLYVECCOMPRESSEDBYTES + SABER_SEEDBYTES);
param int SABER_INDCPA_SECRETKEYBYTES = (SABER_POLYVECBYTES);

param int SABER_PUBLICKEYBYTES = (SABER_INDCPA_PUBLICKEYBYTES);

param int SABER_SECRETKEYBYTES = (SABER_INDCPA_SECRETKEYBYTES + SABER_INDCPA_PUBLICKEYBYTES + SABER_HASHBYTES + SABER_KEYBYTES);

param int SABER_BYTES_CCA_DEC = (SABER_POLYVECCOMPRESSEDBYTES + SABER_SCALEBYTES_KEM);



param int SABER_KN = (SABER_K * SABER_N);
param int SABER_KKN = (SABER_K * SABER_K * SABER_N);
param int N_SB = (SABER_N / 4);
param int N_SB_RES = (2 * N_SB - 1);

param int SHAKE128_RATE = 168;
param int SHAKE256_RATE = 136;
param int SHA3_256_RATE = 136;
param int SHA3_512_RATE = 72;

param int KK13N8 = (SABER_K * SABER_K * (13 * SABER_N / 8));
param int MUNK8 = (SABER_MU * SABER_N * SABER_K / 8);

param int h1 = 4;
param int h2 = 228;




u128 zero_u128 = 0;

u256 zero_u256 = 0;

u256 h1_16u16 = 0x0004000400040004000400040004000400040004000400040004000400040004;
u256 h2_16u16 = 0x00e400e400e400e400e400e400e400e400e400e400e400e400e400e400e400e4;
u256 modp_16u16 = 0x03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff;
u256 modq_16u16 = 0x1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff;

u256 twobit_mask_16u16 = 0x0003000300030003000300030003000300030003000300030003000300030003;
u256 fourbit_mask_16u16 = 0x000f000f000f000f000f000f000f000f000f000f000f000f000f000f000f000f;
u256 sixbit_mask_16u16 = 0x003f003f003f003f003f003f003f003f003f003f003f003f003f003f003f003f;

u256 modq_8u32 = 0x00001fff00001fff00001fff00001fff00001fff00001fff00001fff00001fff;

u256 fourbit_mask_8u32 = 0x0000000f0000000f0000000f0000000f0000000f0000000f0000000f0000000f;
u256 sixteenbit_mask_8u32 = 0x0000ffff0000ffff0000ffff0000ffff0000ffff0000ffff0000ffff0000ffff;

u256 onebit_mask_64u4 = 0x1111111111111111111111111111111111111111111111111111111111111111;
























fn karatsuba_simple(reg ptr u16[N_SB] a_1, reg ptr u16[N_SB] b_1, reg ptr u16[N_SB_RES] result_final) -> reg ptr u16[N_SB_RES]
{
 inline int i;
 inline int j;
 inline int N;

 reg u16 acc1;
 reg u16 acc2;
 reg u16 acc3;
 reg u16 acc4;
 reg u16 acc5;
 reg u16 acc6;
 reg u16 acc7;
 reg u16 acc8;
 reg u16 acc9;
 reg u16 acc10;
 reg u16 t16;

 stack u16[64 / 2 - 1] d01;
 stack u16[64 / 2 - 1] d0123;
 stack u16[64 / 2 - 1] d23;
 stack u16[64 - 1] result_d01;

 N = 64;


 for i = 0 to (N / 2 - 1) {
  d01[i] = 0;
  d0123[i] = 0;
  d23[i] = 0;
 }


 for i = 0 to (N - 1) {
  result_d01[i] = 0;
 }


 for i = 0 to (2 * N - 1) {
  result_final[i] = 0;
 }


 for i = 0 to (N / 4) {
  acc1 = a_1[i];
  acc2 = a_1[i + N / 4];
  acc3 = a_1[i + 2 * N / 4];
  acc4 = a_1[i + 3 * N / 4];

  for j = 0 to (N / 4) {
   acc5 = b_1[j];
   acc6 = b_1[j + N / 4];


   t16 = acc1;
   t16 *= acc5;
   t16 += result_final[i + j];
   result_final[i + j] = t16;


   t16 = acc2;
   t16 *= acc6;
   t16 += result_final[i + j + 2 * N / 4];
   result_final[i + j + 2 * N / 4] = t16;

   acc7 = acc5 + acc6;
   acc8 = acc1 + acc2;


   t16 = acc7;
   t16 *= acc8;
   t16 += d01[i + j];
   d01[i + j] = t16;

   acc7 = b_1[j + 2 * N / 4];
   acc8 = b_1[j + 3 * N / 4];


   t16 = acc7;
   t16 *= acc3;
   t16 += result_final[i + j + 4 * N / 4];
   result_final[i + j + 4 * N / 4] = t16;


   t16 = acc8;
   t16 *= acc4;
   t16 += result_final[i + j + 6 * N / 4];
   result_final[i + j + 6 * N / 4] = t16;

   acc9 = acc3 + acc4;
   acc10 = acc7 + acc8;


   t16 = acc9;
   t16 *= acc10;
   t16 += d23[i + j];
   d23[i + j] = t16;

   acc5 = acc5 + acc7;
   acc7 = acc1 + acc3;


   t16 = acc5;
   t16 *= acc7;
   t16 += result_d01[i + j];
   result_d01[i + j] = t16;

   acc6 = acc6 + acc8;
   acc8 = acc2 + acc4;


   t16 = acc6;
   t16 *= acc8;
   t16 += result_d01[i + j + 2 * N / 4];
   result_d01[i + j + 2 * N / 4] = t16;

   acc5 = acc5 + acc6;
   acc7 = acc7 + acc8;


   t16 = acc5;
   t16 *= acc7;
   t16 += d0123[i + j];
   d0123[i + j] = t16;
  }
 }

 for i = 0 to N / 2 - 1 {

  t16 = d0123[i];
  t16 -= result_d01[i];
  t16 -= result_d01[i + 2 * N / 4];
  d0123[i] = t16;


  t16 = d01[i];
  t16 -= result_final[i];
  t16 -= result_final[i + 2 * N / 4];
  d01[i] = t16;


  t16 = d23[i];
  t16 -= result_final[i + 4 * N / 4];
  t16 -= result_final[i + 6 * N / 4];
  d23[i] = t16;
 }

 for i = 0 to N / 2 -1 {

  t16 = result_d01[i + N / 4];
  t16 += d0123[i];
  result_d01[i + N / 4] = t16;


  t16 = result_final[i + N / 4];
  t16 += d01[i];
  result_final[i + N / 4] = t16;


  t16 = result_final[i + 5 * N / 4];
  t16 += d23[i];
  result_final[i + 5 * N / 4] = t16;
 }

 for i = 0 to N - 1 {

  t16 = result_d01[i];
  t16 -= result_final[i];
  t16 -= result_final[i + N];
  result_d01[i] = t16;
 }

 for i = 0 to N - 1 {

  t16 = result_final[i + N / 2];
  t16 += result_d01[i];
  result_final[i + N / 2] = t16;
 }

 return result_final;
}

inline fn toom_cook_4way(reg ptr u16[SABER_N] a1, reg ptr u16[SABER_N] b1, reg ptr u16[512] result) -> reg ptr u16[512]
{
 inline int i;
 inline int j;

 inline int AB0;
 inline int AB1;
 inline int AB2;
 inline int AB3;

 reg u16 r0;
 reg u16 r1;
 reg u16 r2;
 reg u16 r3;
 reg u16 r4;
 reg u16 r5;
 reg u16 r6;
 reg u16 r7;

 reg u16 t1;
 reg u16 t2;

 reg u32 p1;
 reg u32 p2;

 stack u16[N_SB] aw1;
 stack u16[N_SB] aw2;
 stack u16[N_SB] aw3;
 stack u16[N_SB] aw4;
 stack u16[N_SB] aw5;
 stack u16[N_SB] aw6;
 stack u16[N_SB] aw7;

 stack u16[N_SB] bw1;
 stack u16[N_SB] bw2;
 stack u16[N_SB] bw3;
 stack u16[N_SB] bw4;
 stack u16[N_SB] bw5;
 stack u16[N_SB] bw6;
 stack u16[N_SB] bw7;

 stack u16[N_SB_RES] w1;
 stack u16[N_SB_RES] w2;
 stack u16[N_SB_RES] w3;
 stack u16[N_SB_RES] w4;
 stack u16[N_SB_RES] w5;
 stack u16[N_SB_RES] w6;
 stack u16[N_SB_RES] w7;

 stack ptr u16[512] sresult;

 AB0 = 0;
 AB1 = N_SB;
 AB2 = 2 * N_SB;
 AB3 = 3 * N_SB;

 for i = 0 to N_SB_RES {
  w1[i] = 0;
  w2[i] = 0;
  w3[i] = 0;
  w4[i] = 0;
  w5[i] = 0;
  w6[i] = 0;
  w7[i] = 0;
 }

 for j = 0 to N_SB {
  r0 = a1[AB0 + j];
  r1 = a1[AB1 + j];
  r2 = a1[AB2 + j];
  r3 = a1[AB3 + j];
  r4 = r0 + r2;
  r5 = r1 + r3;
  r6 = r4 + r5;
  r7 = r4;
  r7 -= r5;

  aw3[j] = r6;
  aw4[j] = r7;


  t1 = r0;
  t1 <<= 2;
  t1 += r2;
  t1 <<= 1;
  r4 = t1;


  t1 = r1;
  t1 <<= 2;
  t1 += r3;
  r5 = t1;

  r6 = r4 + r5;
  r7 = r4;
  r7 -= r5;

  aw5[j] = r6;
  aw6[j] = r7;


  t1 = r3;
  t1 <<= 3;
  t2 = r2;
  t2 <<= 2;
  t1 += t2;
  t2 = r1;
  t2 <<= 1;
  t1 += t2;
  t1 += r0;
  r4 = t1;

  aw2[j] = r4;
  aw7[j] = r0;
  aw1[j] = r3;
 }

 for j = 0 to N_SB {
  r0 = b1[AB0 + j];
  r1 = b1[AB1 + j];
  r2 = b1[AB2 + j];
  r3 = b1[AB3 + j];
  r4 = r0 + r2;
  r5 = r1 + r3;
  r6 = r4 + r5;
  r7 = r4;
  r7 -= r5;

  bw3[j] = r6;
  bw4[j] = r7;


  t1 = r0;
  t1 <<= 2;
  t1 += r2;
  t1 <<= 1;
  r4 = t1;


  t1 = r1;
  t1 <<= 2;
  t1 += r3;
  r5 = t1;

  r6 = r4 + r5;
  r7 = r4;
  r7 -= r5;

  bw5[j] = r6;
  bw6[j] = r7;


  t1 = r3;
  t1 <<= 3;
  t2 = r2;
  t2 <<= 2;
  t1 += t2;
  t2 = r1;
  t2 <<= 1;
  t1 += t2;
  t1 += r0;
  r4 = t1;

  bw2[j] = r4;
  bw7[j] = r0;
  bw1[j] = r3;
 }

 sresult = result;

 w1 = karatsuba_simple(aw1, bw1, w1);
 w2 = karatsuba_simple(aw2, bw2, w2);
 w3 = karatsuba_simple(aw3, bw3, w3);
 w4 = karatsuba_simple(aw4, bw4, w4);
 w5 = karatsuba_simple(aw5, bw5, w5);
 w6 = karatsuba_simple(aw6, bw6, w6);
 w7 = karatsuba_simple(aw7, bw7, w7);

 result = sresult;

 for i = 0 to N_SB_RES {
  r0 = w1[i];
  r1 = w2[i];
  r2 = w3[i];
  r3 = w4[i];
  r4 = w5[i];
  r5 = w6[i];
  r6 = w7[i];

  r1 += r4;
  r5 -= r4;


  p1 = (32u) r3;
  p2 = (32u) r2;
  p1 -= p2;
  p1 >>= 1;
  r3 = (16u) p1;

  r4 -= r0;


  t1 = r6;
  t1 <<= 6;
  r4 -= t1;


   r4 <<= 1;
   r4 += r5;

   r2 += r3;


   t1 = r2;
   t1 <<= 6;
   r1 -= t1;
   r1 -= r2;

   r2 -= r6;
   r2 -= r0;


   t1 = 45 * r2;
   r1 += t1;


   p1 = (32u) r2;
   p1 <<= 3;
   p2 = (32u) r4;
   p2 -= p1;
   p2 *= 43691;
   p2 >>= 3;
   r4 = (16u) p2;

   r5 += r1;


   p1 = (32u) r3;
   p1 <<= 4;
   p2 = (32u) r1;
   p2 += p1;
   p2 *= 36409;
   p2 >>= 1;
   r1 = (16u) p2;


   p1 = (32u) r3;
   p2 = (32u) r1;
   p1 += p2;
   p1 *= -1;
   r3 = (16u) p1;


   p1 = (32u) r1;
   p1 *= 30;
   p2 = (32u) r5;
   p1 -= p2;
   p1 *= 61167;
   p1 >>= 2;
   r5 = (16u) p1;

   r2 -= r4;
   r1 -= r5;

   result[i] += r6;
   result[i + 64] += r5;
   result[i + 128] += r4;
   result[i + 192] += r3;
   result[i + 256] += r2;
   result[i + 320] += r1;
   result[i + 384] += r0;
 }

 return result;
}

fn pol_mul_q(reg ptr u16[SABER_N] a, reg ptr u16[SABER_N] b, reg ptr u16[SABER_N] res) -> reg ptr u16[SABER_N]
{
 inline int i;

 reg u16 t16;

 stack u16[512] c;

 stack ptr u16[SABER_N] sres;

 for i = 0 to 512 {
  c[i] = 0;
 }

 sres = res;

 c = toom_cook_4way(a, b, c);

 res = sres;

 for i = SABER_N to 2 * SABER_N {

  t16 = c[i - SABER_N];
  t16 -= c[i];
  res[i - SABER_N] = t16;
  t16 = SABER_Q - 1;
  res[i - SABER_N] &= t16;
 }

 return res;
}

inline fn MatrixVectorMul(reg ptr u16[SABER_KKN] a, reg ptr u16[SABER_KN] skpv, reg ptr u16[SABER_KN] res, reg u16 transpose) -> reg ptr u16[SABER_KN]
{
 inline int i;
 inline int j;
 inline int k;

 reg u16 t;

 stack u16[SABER_N] acc;

 stack ptr u16[SABER_KKN] sa;
 stack ptr u16[SABER_KN] sskpv;
 stack ptr u16[SABER_KN] sres;

 if (transpose == 1) {
  for i = 0 to SABER_K {
   for j = 0 to SABER_K {
    sa = a;
    sskpv = skpv;
    sres = res;

    acc = pol_mul_q(a[j * SABER_KN + i * SABER_N:SABER_N], skpv[j * SABER_N:SABER_N], acc);

    a = sa;
    skpv = sskpv;
    res = sres;

    for k = 0 to SABER_N {
     t = acc[k];

     res[i * SABER_N + k] += t;
     res[i * SABER_N + k] &= (SABER_Q - 1);

     acc[k] = 0;
    }
   }
  }
 } else {
  for i = 0 to SABER_K {
   for j = 0 to SABER_K {
    sa = a;
    sskpv = skpv;
    sres = res;

    acc = pol_mul_q(a[i * SABER_KN + j * SABER_N:SABER_N], skpv[j * SABER_N:SABER_N], acc);

    a = sa;
    skpv = sskpv;
    res = sres;

    for k = 0 to SABER_N {
     t = acc[k];

     res[i * SABER_N + k] += t;
     res[i * SABER_N + k] &= (SABER_Q - 1);

     acc[k] = 0;
    }
   }
  }
 }

 return res;
}






fn pol_mul_p(reg ptr u16[SABER_N] a, reg ptr u16[SABER_N] b, reg ptr u16[SABER_N] res) -> reg ptr u16[SABER_N]
{
 inline int i;

 reg u16 t16;

 stack u16[512] c;

 stack ptr u16[SABER_N] sres;

 for i = 0 to 512 {
  c[i] = 0;
 }

 sres = res;

 c = toom_cook_4way(a, b, c);

 res = sres;

 for i = SABER_N to 2 * SABER_N {

  t16 = c[i - SABER_N];
  t16 -= c[i];
  res[i - SABER_N] = t16;
  t16 = SABER_P - 1;
  res[i - SABER_N] &= t16;
 }

 return res;
}

inline fn InnerProd(reg ptr u16[SABER_KN] pkcl, reg ptr u16[SABER_KN] skpv, reg ptr u16[SABER_N] res) -> reg ptr u16[SABER_N]
{
 inline int j;
 inline int k;

 reg u16 t;

 stack u16[SABER_N] acc;

 stack ptr u16[SABER_KN] spkcl;
 stack ptr u16[SABER_KN] sskpv;
 stack ptr u16[SABER_N] sres;

 for j = 0 to SABER_K {
  spkcl = pkcl;
  sskpv = skpv;
  sres = res;

  acc = pol_mul_p(pkcl[j * SABER_N:SABER_N], skpv[j * SABER_N:SABER_N], acc);

  pkcl = spkcl;
  skpv = sskpv;
  res = sres;

  for k = 0 to SABER_N {
   t = acc[k];

   res[k] += t;
   res[k] &= (SABER_P - 1);

   acc[k] = 0;
  }
 }

 return res;
}







inline fn POL2MSG(stack u16[SABER_N] message_dec_unpacked, stack u8[SABER_KEYBYTES] message_dec) -> stack u8[SABER_KEYBYTES]
{
 inline int i;
 inline int j;

 reg u8 t8;

 reg u16 t16;

 for j = 0 to SABER_KEYBYTES {
  message_dec[j] = 0;

  for i = 0 to 8 {
   t16 = message_dec_unpacked[j * 8 + i];
   t16 <<= i;
   t8 = (8u) t16;
   message_dec[j] |= t8;
  }
 }

 return message_dec;
}























inline fn load64(reg ptr u8[8] x) -> reg u64
{
 inline int i;

 reg u64 r;
 reg u64 t;

 r = (64u) x[0];

 for i = 1 to 8 {
  t = (64u) x[i];
  t <<= (8 * i);
  r |= t;
 }

 return r;
}

fn keccak_absorb_128_32(reg ptr u64[25] s, reg ptr u8[32] m) -> reg ptr u64[25]
{
 inline int i;

 reg u8 t8;

 reg u64 t64;

 stack u8[200] t;



 for i = 0 to SHAKE128_RATE {
  t[i] = 0;
 }

 for i = 0 to 32 {
  t8 = m[i];
  t[i] = t8;
 }

 t[32] = 0x1F;
 t[SHAKE128_RATE - 1] |= 128;

 for i = 0 to (SHAKE128_RATE / 8) {
  t64 = load64(t[8 * i:8]);
  s[i] ^= t64;
 }

 return s;
}











inline fn index(inline int x, inline int y) -> inline int {
  inline int r;

  r = (x % 5) + 5 * (y % 5);

  return r;
}


inline fn ROL64(reg u64 x, inline int c) -> reg u64 {
  reg u64 y;

  _, _, y = #ROL_64(x, c);

  return y;
}


inline fn theta(reg ptr u64[25] a) -> reg ptr u64[25] {
  inline int x, y;

  reg u64[5] c, d;

  for x = 0 to 5 {
    c[x] = 0;
    for y = 0 to 5 {
      c[x] ^= a[x + 5 * y];
    }
  }

  for x = 0 to 5 {
    d[x] = c[(x + 1) % 5];
    _, _, d[x] = #ROL_64(d[x], 1);
    d[x] ^= c[(x + 4) % 5];
  }

  for x = 0 to 5 {
    for y = 0 to 5 {
      a[x + 5 * y] ^= d[x];
    }
  }

  return a;
}


inline fn keccakRhoOffsets(inline int i) -> inline int {
  inline int r, x, y, z, t;

  r = 0;
  x = 1;
  y = 0;
  for t = 0 to 24 {
    if (i == x + 5 * y) {
      r = ((t + 1) * (t + 2) / 2) % 64;
    }
    z = (2 * x + 3 * y) % 5;
    x = y;
    y = z;
  }

  return r;
}


inline fn rho(reg ptr u64[25] a) -> reg ptr u64[25] {
  inline int x, y, i, z;

  for x = 0 to 5 {
    for y = 0 to 5 {
      i = index(x, y);
      z = keccakRhoOffsets(i);
      _, _, a[i] = #ROL_64(a[i], z);
    }
  }

  return a;
}


inline fn pi(reg ptr u64[25] a) -> reg ptr u64[25] {
  inline int x, y, i;

  reg u64 t;

  stack u64[25] b;

  for i = 0 to 25 {
    t = a[i];
    b[i] = t;
  }

  for x = 0 to 5 {
    for y = 0 to 5 {
      t = b[x + 5 * y];
      i = index(y, 2 * x + 3 * y);
      a[i] = t;
    }
  }

  return a;
}


inline fn chi(reg ptr u64[25] a) -> reg ptr u64[25] {
  inline int x, y, i;
  reg u64[5] c;

  for y = 0 to 5 {
    for x = 0 to 5 {
      i = index(x + 1, y);
      c[x] = a[i];
      c[x] = !c[x];
      i = index(x + 2, y);
      c[x] &= a[i];
      i = index(x, y);
      c[x] ^= a[i];
    }

    for x = 0 to 5 {
      a[x + 5 * y] = c[x];
    }

  }
  return a;
}


inline fn iota(reg ptr u64[25] a, reg u64 c) -> reg ptr u64[25] {
  a[0] ^= c;
  return a;
}


inline fn keccakP1600_round(reg ptr u64[25] state, reg u64 c) -> reg ptr u64[25] {
  state = theta(state);
  state = rho(state);
  state = pi(state);
  state = chi(state);
  state = iota(state, c);

  return state;
}

u64[24] KeccakF_RoundConstants = {0x0000000000000001, 0x0000000000008082, 0x800000000000808a, 0x8000000080008000,
                                    0x000000000000808b, 0x0000000080000001, 0x8000000080008081, 0x8000000000008009,
                                    0x000000000000008a, 0x0000000000000088, 0x0000000080008009, 0x000000008000000a,
                                    0x000000008000808b, 0x800000000000008b, 0x8000000000008089, 0x8000000000008003,
                                    0x8000000000008002, 0x8000000000000080, 0x000000000000800a, 0x800000008000000a,
                                    0x8000000080008081, 0x8000000000008080, 0x0000000080000001, 0x8000000080008008};

fn KeccakF1600_StatePermute(reg ptr u64[25] state) -> reg ptr u64[25]
{
  inline int round;

  for round = 0 to 24 {
    state = keccakP1600_round(state, KeccakF_RoundConstants[round]);
  }

  return state;
}





inline fn store64(reg ptr u8[8] x, reg u64 u) -> reg ptr u8[8]
{
 inline int i;

 for i = 0 to 7 {
  x[i] = u;
  u >>= 8;
 }

 x[7] = u;

 return x;
}

inline fn keccak_squeezeblocks_128_KK13N8(reg ptr u8[KK13N8] h, reg ptr u64[25] s) -> reg ptr u8[KK13N8], reg ptr u64[25]
{
 inline int i;
 inline int j;
 inline int iterations;

 reg u64 u;

 iterations = KK13N8 / SHAKE128_RATE;

 for j = 0 to iterations {
  s = KeccakF1600_StatePermute(s);

  for i = 0 to (SHAKE128_RATE / 8) {
   u = s[i];
   h[j * SHAKE128_RATE + 8 * i:8] = store64(h[j * SHAKE128_RATE + 8 * i:8], u);
  }
 }

 return h, s;
}
inline fn keccak_squeezeblocks_128_128(reg ptr u8[SHAKE128_RATE] h, reg ptr u64[25] s) -> reg ptr u8[SHAKE128_RATE], reg ptr u64[25]
{
 inline int i;

 reg u64 u;



 s = KeccakF1600_StatePermute(s);

 for i = 0 to (SHAKE128_RATE / 8) {
  u = s[i];
  h[8 * i:8] = store64(h[8 * i:8], u);
 }

 return h, s;
}

inline fn shake128_KK13N8_32(reg ptr u8[KK13N8] output, reg ptr u8[32] input) -> reg ptr u8[KK13N8]
{
 inline int i;
 inline int nblocks;
 inline int handled;
 inline int remainder;

 reg u8 t8;

 stack u8[SHAKE128_RATE] t;

 stack u64[25] s;

 nblocks = KK13N8 / SHAKE128_RATE;
 handled = nblocks * SHAKE128_RATE;
 remainder = KK13N8 - handled;

 for i = 0 to 25 {
  s[i] = 0;
 }

 s = keccak_absorb_128_32(s, input);

 output, s = keccak_squeezeblocks_128_KK13N8(output, s);


 t, s = keccak_squeezeblocks_128_128(t, s);

 for i = 0 to remainder {
  t8 = t[i];
  output[handled + i] = t8;
 }

 return output;
}







fn BS2POL(reg ptr u8[SABER_POLYBYTES] bytes, reg ptr u16[SABER_N] data) -> reg ptr u16[SABER_N]
{
 inline int j;

 reg u16 b1;
 reg u16 b2;
 reg u16 b3;

 for j = 0 to (SABER_N / 8) {

  b1 = (16u) bytes[13 * j];
  b2 = (16u) bytes[13 * j + 1];
  b1 &= 0xff;
  b2 &= 0x1f;
  b2 <<= 8;
  b1 |= b2;
  data[8 * j] = b1;


  b1 = (16u) bytes[13 * j + 1];
  b2 = (16u) bytes[13 * j + 2];
  b3 = (16u) bytes[13 * j + 3];
  b1 >>= 5;
  b1 &= 0x07;
  b2 &= 0xff;
  b2 <<= 3;
  b3 &= 0x03;
  b3 <<= 11;
  b2 |= b3;
  b1 |= b2;
  data[8 * j + 1] = b1;


  b1 = (16u) bytes[13 * j + 3];
  b2 = (16u) bytes[13 * j + 4];
  b1 >>= 2;
  b1 &= 0x3f;
  b2 &= 0x7f;
  b2 <<= 6;
  b1 |= b2;
  data[8 * j + 2] = b1;


  b1 = (16u) bytes[13 * j + 4];
  b2 = (16u) bytes[13 * j + 5];
  b3 = (16u) bytes[13 * j + 6];
  b1 >>= 7;
  b1 &= 0x01;
  b2 &= 0xff;
  b2 <<= 1;
  b3 &= 0x0f;
  b3 <<= 9;
  b2 |= b3;
  b1 |= b2;
  data[8 * j + 3] = b1;


  b1 = (16u) bytes[13 * j + 6];
  b2 = (16u) bytes[13 * j + 7];
  b3 = (16u) bytes[13 * j + 8];
  b1 >>= 4;
  b1 &= 0x0f;
  b2 &= 0xff;
  b2 <<= 4;
  b3 &= 0x01;
  b3 <<= 12;
  b2 |= b3;
  b1 |= b2;
  data[8 * j + 4] = b1;


  b1 = (16u) bytes[13 * j + 8];
  b2 = (16u) bytes[13 * j + 9];
  b1 >>= 1;
  b1 &= 0x7f;
  b2 &= 0x3f;
  b2 <<= 7;
  b1 |= b2;
  data[8 * j + 5] = b1;


  b1 = (16u) bytes[13 * j + 9];
  b2 = (16u) bytes[13 * j + 10];
  b3 = (16u) bytes[13 * j + 11];
  b1 >>= 6;
  b1 &= 0x03;
  b2 &= 0xff;
  b2 <<= 2;
  b3 &= 0x07;
  b3 <<= 10;
  b2 |= b3;
  b1 |= b2;
  data[8 * j + 6] = b1;


  b1 = (16u) bytes[13 * j + 11];
  b2 = (16u) bytes[13 * j + 12];
  b1 >>= 3;
  b1 &= 0x1f;
  b2 &= 0xff;
  b2 <<= 5;
  b1 |= b2;
  data[8 * j + 7] = b1;
 }

 return data;
}

fn GenMatrix(reg ptr u16[SABER_KKN] a, reg ptr u8[SABER_SEEDBYTES] seed) -> reg ptr u16[SABER_KKN]
{
 inline int i;
 inline int j;
 inline int k;

 reg u16 t16;

 stack u8[KK13N8] buf;

 stack u16[SABER_N] temp_ar;

 stack ptr u16[SABER_KKN] sa;

 sa = a;

 buf = shake128_KK13N8_32(buf, seed);

 a = sa;

 for i = 0 to SABER_K {
  for j = 0 to SABER_K {
   temp_ar = BS2POL(buf[(i * SABER_K + j) * 13 * SABER_N / 8:SABER_POLYBYTES], temp_ar);

   for k = 0 to SABER_N {
    t16 = temp_ar[k];
    t16 &= (SABER_Q - 1);
    a[i * SABER_KN + j * SABER_N + k] = t16;
   }
  }
 }

 return a;
}






inline fn shake128_32_32(reg ptr u8[32] output, reg ptr u8[32] input) -> reg ptr u8[32]
{
 inline int i;

 reg u8 t8;

 stack u8[SHAKE128_RATE] t;

 stack u64[25] s;

 for i = 0 to 25 {
  s[i] = 0;
 }

 s = keccak_absorb_128_32(s, input);




 t, s = keccak_squeezeblocks_128_128(t, s);

 for i = 0 to 32 {
  t8 = t[i];
  output[i] = t8;
 }

 return output;
}













inline fn keccak_squeezeblocks_128_MUNK8(reg ptr u8[MUNK8] h, reg ptr u64[25] s) -> reg ptr u8[MUNK8], reg ptr u64[25]
{
 inline int i;
 inline int j;
 inline int iterations;

 reg u64 u;

 iterations = MUNK8 / SHAKE128_RATE;

 for j = 0 to iterations {
  s = KeccakF1600_StatePermute(s);

  for i = 0 to (SHAKE128_RATE / 8) {
   u = s[i];
   h[j * SHAKE128_RATE + 8 * i:8] = store64(h[j * SHAKE128_RATE + 8 * i:8], u);
  }
 }

 return h, s;
}


inline fn shake128_MUNK8_32(reg ptr u8[MUNK8] output, reg ptr u8[32] input) -> reg ptr u8[MUNK8]
{
 inline int i;
 inline int nblocks;
 inline int handled;
 inline int remainder;

 reg u8 t8;

 stack u8[SHAKE128_RATE] t;

 stack u64[25] s;

 nblocks = MUNK8 / SHAKE128_RATE;
 handled = nblocks * SHAKE128_RATE;
 remainder = MUNK8 - handled;

 for i = 0 to 25 {
  s[i] = 0;
 }

 s = keccak_absorb_128_32(s, input);

 output, s = keccak_squeezeblocks_128_MUNK8(output, s);


 t, s = keccak_squeezeblocks_128_128(t, s);

 for i = 0 to remainder {
  t8 = t[i];
  output[handled + i] = t8;
 }

 return output;
}







fn cbd(reg ptr u16[SABER_N] r, reg ptr u8[SABER_N] buf) -> reg ptr u16[SABER_N]
{
 inline int i;

 reg u8 t8;

 reg u16 t16;

 reg u32 t32;
 reg u32 d;
 reg u32 tt;

 stack u32[4] a;
 stack u32[4] b;

 for i = 0 to (SABER_N / 4) {

        t32 = (32u) buf[4 * i];

        tt = (32u) buf[4 * i + 1];
        tt <<= 8;
        t32 |= tt;

        tt = (32u) buf[4 * i + 2];
        tt <<= 16;
        t32 |= tt;

        tt = (32u) buf[4 * i + 3];
        tt <<= 24;
        t32 |= tt;


  tt = t32;
        tt &= 0x11111111;
  d = tt;

  tt = t32;
  tt >>= 1;
  tt &= 0x11111111;
  d += tt;

  tt = t32;
  tt >>= 2;
  tt &= 0x11111111;
  d += tt;

  tt = t32;
  tt >>= 3;
  tt &= 0x11111111;
  d += tt;


     t32 = d;
     t32 &= 0xf;
  a[0] = t32;


     t32 = d;
     t32 >>= 4;
     t32 & = 0xf;
     b[0] = t32;


     t32 = d;
     t32 >>= 8;
     t32 &= 0xf;
     a[1] = t32;


     t32 = d;
     t32 >>= 12;
     t32 &= 0xf;
     b[1] = t32;


     t32 = d;
     t32 >>= 16;
     t32 &= 0xf;
     a[2] = t32;


  t32 = d;
     t32 >>= 20;
     t32 &= 0xf;
     b[2] = t32;


     t32 = d;
     t32 >>= 24;
     t32 &= 0xf;
     a[3] = t32;


     t32 = d;
     t32 >>= 28;
     b[3] = t32;


     t32 = a[0];
     t32 -= b[0];
     t16 = (16u) t32;

     r[4 * i] = t16;


     t32 = a[1];
     t32 -= b[1];
     t16 = (16u) t32;

     r[4 * i + 1] = t16;


     t32 = a[2];
     t32 -= b[2];
     t16 = (16u) t32;

     r[4 * i + 2] = t16;


     t32 = a[3];
     t32 -= b[3];
     t16 = (16u) t32;

     r[4 * i + 3] = t16;
 }

 return r;
}

fn GenSecret(reg ptr u16[SABER_KN] r, reg ptr u8[SABER_COINBYTES] seed) -> reg ptr u16[SABER_KN]
{
 inline int i;

 stack u8[MUNK8] buf;

 stack ptr u16[SABER_KN] sr;

 sr = r;

 buf = shake128_MUNK8_32(buf, seed);

 r = sr;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = cbd(r[i * SABER_N:SABER_N], buf[i * SABER_MU * SABER_N / 8:SABER_N]);
 }

 return r;
}







inline fn POLVECq2BS(reg ptr u8[SABER_POLYVECBYTES] bytes, reg ptr u16[SABER_KN] data) -> reg ptr u8[SABER_POLYVECBYTES]
{
 inline int i;
 inline int j;

 reg u16 d1;
 reg u16 d2;

 for i = 0 to SABER_K {
  for j = 0 to (SABER_N / 8) {

   d1 = data[i * SABER_N + 8 * j];
   d1 &= 0xff;
   bytes[i * (SABER_N * 13) / 8 + 13 * j] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j];
   d2 = data[i * SABER_N + 8 * j + 1];
   d1 >>= 8;
   d1 &= 0x1f;
   d2 &= 0x07;
   d2 <<= 5;
   d1 |= d2;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 1] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 1];
   d1 >>= 3;
   d1 &= 0xff;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 2] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 1];
   d2 = data[i * SABER_N + 8 * j + 2];
   d1 >>= 11;
   d1 &= 0x03;
   d2 &= 0x3f;
   d2 <<= 2;
   d1 |= d2;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 3] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 2];
   d2 = data[i * SABER_N + 8 * j + 3];
   d1 >>= 6;
   d1 &= 0x7f;
   d2 &= 0x01;
   d2 <<= 7;
   d1 |= d2;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 4] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 3];
   d1 >>= 1;
   d1 &= 0xff;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 5] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 3];
   d2 = data[i * SABER_N + 8 * j + 4];
   d1 >>= 9;
   d1 &= 0x0f;
   d2 &= 0x0f;
   d2 <<= 4;
   d1 |= d2;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 6] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 4];
   d1 >>= 4;
   d1 &= 0xff;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 7] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 4];
   d2 = data[i * SABER_N + 8 * j + 5];
   d1 >>= 12;
   d1 &= 0x01;
   d2 &= 0x7f;
   d2 <<= 1;
   d1 |= d2;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 8] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 5];
   d2 = data[i * SABER_N + 8 * j + 6];
   d1 >>= 7;
   d1 &= 0x3f;
   d2 &= 0x03;
   d2 <<= 6;
   d1 |= d2;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 9] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 6];
   d1 >>= 2;
   d1 &= 0xff;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 10] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 6];
   d2 = data[i * SABER_N + 8 * j + 7];
   d1 >>= 10;
   d1 &= 0x07;
   d2 &= 0x1f;
   d2 <<= 3;
   d1 |= d2;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 11] = (8u) d1;


   d1 = data[i * SABER_N + 8 * j + 7];
   d1 >>= 5;
   d1 &= 0xff;
   bytes[i * (SABER_N * 13) / 8 + 13 * j + 12] = (8u) d1;
  }
 }

 return bytes;
}







fn POLVECp2BS(reg ptr u8[SABER_POLYVECCOMPRESSEDBYTES] bytes, reg ptr u16[SABER_KN] data) -> reg ptr u8[SABER_POLYVECCOMPRESSEDBYTES]
{
 inline int i;
 inline int j;

 reg u16 d1;
 reg u16 d2;

 for i = 0 to SABER_K {
  for j = 0 to (SABER_N / 4) {

   d1 = data[i * SABER_N + 4 * j];
   d1 &= 0xff;
   bytes[i * (SABER_N * 10) / 8 + 5 * j] = (8u) d1;


   d1 = data[i * SABER_N + 4 * j];
   d2 = data[i * SABER_N + 4 * j + 1];
   d1 >>= 8;
   d1 &= 0x03;
   d2 &= 0x3f;
   d2 <<= 2;
   d1 |= d2;
   bytes[i * (SABER_N * 10) / 8 + 5 * j + 1] = (8u) d1;


   d1 = data[i * SABER_N + 4 * j + 1];
   d2 = data[i * SABER_N + 4 * j + 2];
   d1 >>= 6;
   d1 &= 0x0f;
   d2 &= 0x0f;
   d2 <<= 4;
   d1 |= d2;
   bytes[i * (SABER_N * 10) / 8 + 5 * j + 2] = (8u) d1;


   d1 = data[i * SABER_N + 4 * j + 2];
   d2 = data[i * SABER_N + 4 * j + 3];
   d1 >>= 4;
   d1 &= 0x3f;
   d2 &= 0x03;
   d2 <<= 6;
   d1 |= d2;
   bytes[i * (SABER_N * 10) / 8 + 5 * j + 3] = (8u) d1;


   d1 = data[i * SABER_N + 4 * j + 3];
   d1 >>= 2;
   d1 &= 0xff;
   bytes[i * (SABER_N * 10) / 8 + 5 * j + 4] = (8u) d1;
  }
 }

 return bytes;
}












param int _16XP = 0;
param int _16XPINV = 16;
param int _16XV = 32;
param int _16XSHIFT = 48;
param int _16XMONT_PINV = 64;
param int _16XMONT = 80;
param int _16XF_PINV = 96;
param int _16XF = 112;
param int _ZETAS = 128;
param int _TWIST32 = 288;
param int _TWIST4 = 800;


param int _REVIDXW = 0;
param int _REVIDXD = 32;
param int _SIGNMSKW = 64;

u16 P0 = 7681;
u16 P1 = 10753;
u16 CRT_U_PINV = 32747;
u16 CRT_U = 3563;


param int P_0 = 7681;
param int PINV_0 = -7679;
param int MONT_0 = -3593;
param int MONT_PINV_0 = -9;
param int V_0 = 17474;
param int SHIFT_0 = 16;
param int F_0 = 1912;
param int F_PINV_0 = -2184;

u16[864] PDATA0 = {

 P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0,


 PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0,
 PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0,


 V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0,


 SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0,
 SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0,


 MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0,
 MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0,


 MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0,
 MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0,


 F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0,
 F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0,


 F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0,


   28865, 28865, 28865, 28865, 28865, 28865, 28865, 28865,
   28865, 28865, 28865, 28865, 28865, 28865, 28865, 28865,
    3777, 3777, 3777, 3777, 3777, 3777, 3777, 3777,
    3777, 3777, 3777, 3777, 3777, 3777, 3777, 3777,
  -10350, -10350, -10350, -10350, -10350, -10350, -10350, -10350,
  -10350, -10350, -10350, -10350, -10350, -10350, -10350, -10350,
   -3182, -3182, -3182, -3182, -3182, -3182, -3182, -3182,
   -3182, -3182, -3182, -3182, -3182, -3182, -3182, -3182,
    4496, 4496, 4496, 4496, 4496, 4496, 4496, 4496,
   -7244, -7244, -7244, -7244, -7244, -7244, -7244, -7244,
   -3696, -3696, -3696, -3696, -3696, -3696, -3696, -3696,
   -1100, -1100, -1100, -1100, -1100, -1100, -1100, -1100,
   16425, 16425, 16425, 16425, 16425, 16425, 16425, 16425,
   16425, 16425, 16425, 16425, 16425, 16425, 16425, 16425,
    3625, 3625, 3625, 3625, 3625, 3625, 3625, 3625,
    3625, 3625, 3625, 3625, 3625, 3625, 3625, 3625,
   14744, 14744, 14744, 14744, 14744, 14744, 14744, 14744,
   -4974, -4974, -4974, -4974, -4974, -4974, -4974, -4974,
    2456, 2456, 2456, 2456, 2456, 2456, 2456, 2456,
    2194, 2194, 2194, 2194, 2194, 2194, 2194, 2194,


      -9, -529, 32738, -1851, -9, 29394, -7508, -20435,
      -9, 26288, 9855, -19215, -9, 16006, -12611, -964,
   -3593, -17, -1054, 3781, -3593, 3794, 2732, -2515,
   -3593, 1712, 2175, -3343, -3593, -3450, -2883, 1084,
   16279, 26288, -8558, -6297, 11783, -25648, 14351, -25733,
   21066, -23882, -17440, -7304, -26279, 16791, 22124, -20435,
   -3689, 1712, -1390, -1689, 7, -1072, -1521, 1403,
    -438, -2378, -1056, -3208, 1881, -3177, -404, -2515,
    2816, -22039, 9855, 21168, -19394, 30255, -27132, 17013,
   23489, -18506, 1869, 10145, -3114, 9650, -15358, -24232,
    2816, -2071, 2175, -3408, -1986, -2001, 3588, -1931,
   -1599, 2998, 3405, 1441, 2006, 434, 2, -3752,
    1724, -24214, 6032, -19215, -21467, 29453, -16655, 32124,
    4505, 13686, -25946, -12790, -23668, -31518, 14351, 12449,
    3772, 3434, -2160, -3343, 549, -1779, -783, 1404,
    -103, 2422, 3750, -1526, 2956, 226, -1521, 3745,
  -11655, -1715, 24743, 26766, 23754, 22943, -2722, 4880,
   18242, 26621, -32329, -10333, -22593, -16715, 30426, 2858,
     121, -179, -3417, 3214, 2250, -1121, -1698, -3312,
     834, 3581, -3145, -3677, 2495, -2891, 730, -2262,
   21066, -4624, -24573, -16186, 29667, -30597, 23225, 10333,
  -15998, 6510, -3558, 17491, 11792, 30255, -4693, 21723,
    -438, 3568, -1533, -2874, 3555, -3461, 2233, 3677,
    -638, -658, -486, -429, 3600, -2001, -2133, -293,
  -20469, -23882, 26663, 14718, -9488, -16885, -26220, 17636,
  -19351, -17082, 2722, 2807, 10972, -5990, 29871, -5299,
   -1525, -2378, -1497, -642, -1296, 2059, -3692, -796,
     617, -3770, 1698, -777, -3364, -2918, -2385, -3763,
   -4983, 18745, -17440, -32695, -4505, -12261, -32252, 23933,
    2073, -30938, 30136, 16083, -21467, -32414, -8908, -947,
   -1399, -2247, -1056, 3657, 103, -1509, -1532, 893,
   -2535, -1242, 1464, -1837, 549, -670, -2764, 589,
      -9, -1851, -8558, -22039, -9, 4573, -26441, 16791,
      -9, -6297, 6032, -4624, -9, -9513, 9360, 16006,
   -3593, 3781, -1390, -2071, -3593, -2083, 2743, -3177,
   -3593, -1689, -2160, 3568, -3593, 3287, 1168, -3450,
    1724, -19215, 24743, -4624, -21766, 1007, -15358, -25648,
   -4983, -7304, -16092, -13711, 21399, 4573, -12611, 29394,
    3772, -3343, -3417, 3568, -2310, 1519, 2, -1072,
   -1399, -3208, -1756, 2161, 1431, -2083, -2883, 3794,
  -20469, 14718, -17440, 16638, -15307, 12449, 12269, -22764,
  -26382, -5452, -25946, -11996, 5759, -964, -26441, 9087,
   -1525, -642, -1056, 1278, -1483, 3745, -2579, -236,
   -2830, 692, 3750, 2340, -1921, 1084, 2743, 1407,
    5930, -23933, -16092, -18506, 11792, -28805, -27132, -5990,
   -5913, 27243, -13933, 6510, -26279, -6766, -7508, 16791,
     810, -893, -1756, 2998, 3600, -1669, 3588, -2918,
   -1305, -2965, 915, -658, 1881, 402, 2732, -3177,
  -18191, -15221, -26262, 2739, -828, -15145, -8908, -9633,
   20315, -15111, -10478, 802, -20870, -4565, 22124, 26783,
   -2319, 3723, 1386, 1203, -2876, -2345, -2764, -929,
   -1701, -3335, -3310, -222, -1414, -2005, -404, 2719,
    4505, -5452, -3456, -28958, -14121, 32124, 17602, 2526,
    2073, 22790, -24052, 9633, -21766, -20435, 21868, 3524,
    -103, 692, -3456, 2786, -1321, 1404, 194, 3550,
   -2535, 3334, 2572, 929, -2310, -2515, -660, 1476,
    7491, -12790, -22875, 16885, 22568, 27858, 10478, 20119,
   31177, 5299, -21860, -10495, -3114, 1007, 8472, 9650,
   -2237, -1526, -859, -2059, 2088, 2258, 3310, 151,
    1993, 3763, -3428, -2815, 2006, 1519, -3816, 434,
   -5913, 27636, -32329, -2952, 29667, 23984, -10409, 8831,
  -11792, 14138, 13541, 31518, 11783, 30844, -15358, -19274,
   -1305, 1012, -3145, 1144, 3555, -592, 2391, 1151,
   -3600, 826, 2789, -226, 7, 124, 2, 2230,


      -9, -16425, -28865, 10350, -3593, -3625, -3777, 3182,
      -9, -10350, 28865, 16425, -3593, -3182, 3777, 3625,
      -9, 4496, -10350, 14744, -3593, -3696, -3182, 2456,
      -9, 4974, -16425, 7244, -3593, -2194, -3625, 1100,
      -9, -11655, 4496, -18191, -3593, 121, -3696, -2319,
      -9, -22593, 7244, -20315, -3593, 2495, 1100, 1701,
      -9, -18191, 14744, -23754, -3593, -2319, 2456, -2250,
      -9, -20870, 4974, -22593, -3593, -1414, -2194, 2495
};



param int P_1 = 10753;
param int MONT_1 = 1018;
param int MONT_PINV_1 = -6;
param int PINV_1 = -10751;
param int V_1 = 12482;
param int SHIFT_1 = 16;
param int F_1 = 2536;
param int F_PINV_1 = -1560;


u16[864] PDATA1 = {

 P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1,


 PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1,
 PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1,


 V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1,


 SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1,
 SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1,


 MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1,
 MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1,


 MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1,
 MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1,


 F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1,
 F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1,


 F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1,


   27359, 27359, 27359, 27359, 27359, 27359, 27359, 27359,
   27359, 27359, 27359, 27359, 27359, 27359, 27359, 27359,
     223, 223, 223, 223, 223, 223, 223, 223,
     223, 223, 223, 223, 223, 223, 223, 223,
   -1956, -1956, -1956, -1956, -1956, -1956, -1956, -1956,
   -1956, -1956, -1956, -1956, -1956, -1956, -1956, -1956,
    4188, 4188, 4188, 4188, 4188, 4188, 4188, 4188,
    4188, 4188, 4188, 4188, 4188, 4188, 4188, 4188,
   10093, 10093, 10093, 10093, 10093, 10093, 10093, 10093,
  -21094, -21094, -21094, -21094, -21094, -21094, -21094, -21094,
    2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413,
   -3686, -3686, -3686, -3686, -3686, -3686, -3686, -3686,
     408, 408, 408, 408, 408, 408, 408, 408,
     408, 408, 408, 408, 408, 408, 408, 408,
   -3688, -3688, -3688, -3688, -3688, -3688, -3688, -3688,
   -3688, -3688, -3688, -3688, -3688, -3688, -3688, -3688,
   28517, 28517, 28517, 28517, 28517, 28517, 28517, 28517,
  -20856, -20856, -20856, -20856, -20856, -20856, -20856, -20856,
     357, 357, 357, 357, 357, 357, 357, 357,
    -376, -376, -376, -376, -376, -376, -376, -376,


      -6, -61, -609, -6095, -6, 14237, -31235, 23836,
      -6, -19643, -2017, -13811, -6, 27329, 11300, -7722,
    1018, -573, 5023, -3535, 1018, -1635, 2045, -2788,
    1018, 1349, 3615, -5107, 1018, 5313, 5156, -554,
    4589, -19643, 177, 1767, 24098, 1725, -31418, -7801,
  -12378, 16236, 31558, 232, 22209, 29644, -18845, 23836,
   -3091, 1349, 2737, -4889, -3550, 2237, 326, 1927,
    2982, -2196, -2234, 4328, 193, -5172, -2973, -2788,
   17675, -19863, -2017, -20173, 4547, -4083, -29364, -21593,
   25543, 11123, 512, 11623, 7429, -21161, -11555, -24129,
    4875, -5015, 3615, 3891, 4035, 4621, 1356, 4519,
    2503, 2419, 512, 4967, -4347, -3241, 5341, -2113,
   -5126, 14280, 11726, -13811, -20490, 24025, -24037, -13024,
  -27152, -19564, -8801, 12415, -6381, -26286, -31418, -23952,
   -4102, 1992, -1586, -5107, 3062, -2087, 4123, 3360,
   -2576, -1132, -3169, 1663, 1299, 3410, 326, 624,
   -7033, -4797, 17571, -20899, 16090, 31583, 16614, -13164,
  -29449, -19454, 17096, -16809, -12476, -26292, -4090, -12653,
    2695, -5309, 675, -4003, 730, 4447, -794, 5268,
    4855, 2050, 4808, 1111, -2236, 4428, -5114, -4973,
  -12378, 7289, 7356, 8027, 15864, -31467, -24976, 16809,
   22532, 6747, -13012, 4967, -20198, -4083, 25555, -31497,
    2982, -2439, -2884, 3419, -4616, -2283, -400, -1111,
       4, 2139, 1324, -1689, -2790, 4621, 467, 2807,
   14731, 16236, 31290, -14780, -10001, 32351, -7795, -9691,
   18363, 5729, -16614, -4248, 3639, 3346, 4394, 22483,
    1931, -2196, -454, -4540, 3823, 5215, 909, -5083,
   -2629, 97, 794, -152, 5175, 274, -2774, -2605,
  -16724, 29370, 31558, -12098, 27152, 12336, 19844, -22215,
    5766, -29827, 7856, 23093, -20490, -3035, -21892, -8935,
   -2388, -2374, -2234, -834, 2576, 4144, -2684, 825,
    4742, 3453, -336, 3125, 3062, 1573, 636, -2279,
      -6, -6095, 177, -19863, -6, -18077, -7326, 29644,
      -6, 1767, 11726, 7289, -6, -19661, 11141, 27329,
    1018, -3535, 2737, -5015, 1018, -2205, -2206, -5172,
    1018, -4889, -1586, -2439, 1018, 4403, -635, 5313,
   -5126, -13811, 17571, 7289, -23781, -18918, -11555, 1725,
  -16724, 232, -1627, 13158, 15840, -18077, 11300, 14237,
   -4102, -5107, 675, -2439, 4379, -1510, 5341, 2237,
   -2388, 4328, 2981, -4250, -544, -2205, 5156, -1635,
   14731, -14780, 31558, -30144, 3925, -23952, -780, -20070,
  -14847, -19856, -8801, -3699, -11683, -7722, -7326, 25482,
    1931, -4540, -2234, 2624, 341, 624, 1268, -2662,
   -4095, 4720, -3169, 5005, 5213, -554, -2206, 1930,
    2316, 22215, -1627, 11123, -20198, -6594, -29364, 3346,
   24269, -25652, -31887, 6747, 22209, 15328, -31235, 29644,
     268, -825, 2981, 2419, -2790, 4670, 1356, 274,
     205, 5068, 3441, 2139, 193, -1056, 2045, -5172,
  -18345, 5120, 7716, -17394, 28224, 24165, -21892, 14329,
    9508, -4717, -8246, 32070, 16072, 8161, -18845, 24330,
    -425, 5120, 1572, 2062, -4544, -3995, 636, 4601,
    3364, 2963, 970, -1722, 3784, 2529, -2973, 778,
  -27152, -19856, 969, -13987, 31217, -13024, -29407, 7880,
    5766, 31924, -17352, -14329, -23781, 23836, 22044, 8758,
   -2576, 4720, -567, 2909, 1009, 3360, -2271, -4408,
    4742, 1204, -5064, -4601, 4379, -2788, -4580, -458,
  -28103, 12415, 28541, -32351, -23056, -30467, 8246, 12976,
   26518, -22483, 32076, 3998, 7429, -18918, -14999, -21161,
   -5063, 1663, -3715, -5215, 1520, 2813, -970, 4784,
     918, 2605, -2740, -1122, -4347, -1510, -151, -3241,
   24269, 20661, 17096, -9343, 15864, -951, -1932, -28712,
   20198, -24641, 2395, 26286, 24098, 15517, -11555, 11952,
     205, 693, 4808, 1409, -4616, -2487, 116, -40,
    2790, -2625, -2213, -3410, -3550, -355, 5341, 3760,


      -6, -408, -27359, 1956, 1018, 3688, -223, -4188,
      -6, -1956, 27359, 408, 1018, 4188, 223, -3688,
      -6, 10093, -1956, 28517, 1018, 2413, 4188, 357,
      -6, 20856, -408, 21094, 1018, 376, 3688, 3686,
      -6, -7033, 10093, -18345, 1018, 2695, 2413, -425,
      -6, -12476, 21094, -9508, 1018, -2236, 3686, -3364,
      -6, -18345, 28517, -16090, 1018, -425, 357, -730,
      -6, 16072, 20856, -12476, 1018, 3784, 376, -2236
};












inline fn shuffle8(reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3) -> reg u256, reg u256
{
 r2 = #VPERM2I128(r0, r1, 0x20);
 r3 = #VPERM2I128(r0, r1, 0x31);

 return r2, r3;
}





inline fn shuffle4(reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3) -> reg u256, reg u256
{
 r2 = #VPUNPCKL_4u64(r0, r1);
 r3 = #VPUNPCKH_4u64(r0, r1);

 return r2, r3;
}





inline fn shuffle2(reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3) -> reg u256, reg u256, reg u256
{
 r2 = #VMOVSLDUP_8u32(r1);
 r2 = #VPBLEND_8u32(r0, r2, 0xAA);
 r0 >>4u64= 32;
 r3 = #VPBLEND_8u32(r0, r1, 0xAA);

 return r0, r2, r3;
}





inline fn shuffle1(reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3) -> reg u256, reg u256, reg u256
{
 r2 = #VPSLL_8u32(r1, 16);
 r2 = #VPBLEND_16u16(r0, r2, 0xAA);
 r0 = #VPSRL_8u32(r0, 16);
 r3 = #VPBLEND_16u16(r0, r1, 0xAA);

 return r0, r2, r3;
}





inline fn fqmulprecomp_0(reg u256 al, reg u256 ah, reg u256 b, reg u256 x, reg u256 ymm0) -> reg u256, reg u256
{
 x = #VPMULL_16u16(b, al);
 b = #VPMULH_16u16(b, ah);
 x = #VPMULH_16u16(x, ymm0);

 b -16u16= x;

 return b, x;
}

inline fn fqmulprecomp_1(reg u256 al, reg u256 ah, reg u256 b, reg u256 x, reg u256 ymm0) -> reg u256, reg u256
{
 x = #VPMULL_16u16(b, al);
 b = #VPMULH_16u16(b, ah);
 x = #VPMULH_16u16(x, ymm0);

 b = x -16u16 b;

 return b, x;
}





inline fn fqmulprecomp1(inline int off, reg u256 b, reg u256 x, reg u256 ymm0, reg ptr u16[864] pdata) -> reg u256, reg u256
{
 x = #VPMULL_16u16(b, pdata[u256 off * 2 / 32]);
 b = #VPMULH_16u16(b, pdata[u256 (off + 16) * 2 / 32]);
 x = #VPMULH_16u16(x, ymm0);

 b -16u16= x;

 return b, x;
}


inline fn update_0(reg u256 rln, reg u256 rl0, reg u256 rl1, reg u256 rl2, reg u256 rl3, reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 rln = rl0 +16u16 rh0;
 rh0 = rl0 -16u16 rh0;

 rl0 = rl1 +16u16 rh1;
 rh1 = rl1 -16u16 rh1;

 rl1 = rl2 +16u16 rh2;
 rh2 = rl2 -16u16 rh2;

 rl2 = rl3 +16u16 rh3;
 rh3 = rl3 -16u16 rh3;

 return rln, rl0, rl1, rl2, rh0, rh1, rh2, rh3;
}


inline fn update_1(reg u256 rln, reg u256 rl0, reg u256 rl1, reg u256 rl2, reg u256 rl3, reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3, reg u256 ymm12, reg u256 ymm13, reg u256 ymm14, reg u256 ymm15)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 rln = rl0 +16u16 rh0;
 rh0 = rl0 -16u16 rh0;

 rl0 = rl1 +16u16 rh1;
 rh1 = rl1 -16u16 rh1;

 rl1 = rl2 +16u16 rh2;
 rh2 = rl2 -16u16 rh2;

 rl2 = rl3 +16u16 rh3;
 rh3 = rl3 -16u16 rh3;

 rln -16u16= ymm12;
 rh0 +16u16= ymm12;

 rl0 -16u16= ymm13;
 rh1 +16u16= ymm13;

 rl1 -16u16= ymm14;
 rh2 +16u16= ymm14;

 rl2 -16u16= ymm15;
 rh3 +16u16= ymm15;

 return rln, rl0, rl1, rl2, rh0, rh1, rh2, rh3;
}


inline fn reduce_0(reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3, reg u256 ymm0, reg u256 ymm12, reg u256 ymm13, reg u256 ymm14, reg u256 ymm15)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 ymm12 = #VPMULH_16u16(ymm0, ymm12);
 ymm13 = #VPMULH_16u16(ymm0, ymm13);

 ymm14 = #VPMULH_16u16(ymm0, ymm14);
 ymm15 = #VPMULH_16u16(ymm0, ymm15);

 rh0 -16u16= ymm12;
 rh1 -16u16= ymm13;
 rh2 -16u16= ymm14;
 rh3 -16u16= ymm15;

 return rh0, rh1, rh2, rh3, ymm12, ymm13, ymm14, ymm15;
}


inline fn reduce_1(reg u256 ymm0, reg u256 ymm12, reg u256 ymm13, reg u256 ymm14, reg u256 ymm15) -> reg u256, reg u256, reg u256, reg u256
{
 ymm12 = #VPMULH_16u16(ymm0, ymm12);
 ymm13 = #VPMULH_16u16(ymm0, ymm13);

 ymm14 = #VPMULH_16u16(ymm0, ymm14);
 ymm15 = #VPMULH_16u16(ymm0, ymm15);

 return ymm12, ymm13, ymm14, ymm15;
}

inline fn mul(reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3, reg u256 zl0, reg u256 zl1, reg u256 zh0, reg u256 zh1)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 reg u256 ymm12, ymm13, ymm14, ymm15;

 ymm12 = #VPMULL_16u16(zl0, rh0);
 ymm13 = #VPMULL_16u16(zl0, rh1);

 ymm14 = #VPMULL_16u16(zl1, rh2);
 ymm15 = #VPMULL_16u16(zl1, rh3);

 rh0 = #VPMULH_16u16(zh0, rh0);
 rh1 = #VPMULH_16u16(zh0, rh1);

 rh2 = #VPMULH_16u16(zh1, rh2);
 rh3 = #VPMULH_16u16(zh1, rh3);

 return rh0, rh1, rh2, rh3, ymm12, ymm13, ymm14, ymm15;
}

inline fn level0(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a, inline int off, reg u256 ymm0, reg u256 ymm1, reg u256 ymm2) -> reg ptr u16[SABER_N]
{
 reg u256 ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;

 ymm8 = a[u256 (64 * off + 128) * 2 / 32];
 ymm9 = a[u256 (64 * off + 144) * 2 / 32];
 ymm10 = a[u256 (64 * off + 160) * 2 / 32];
 ymm11 = a[u256 (64 * off + 176) * 2 / 32];


 ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15 = mul(ymm8, ymm9, ymm10, ymm11, ymm1, ymm1, ymm2, ymm2);

 ymm4 = a[u256 (64 * off + 0) * 2 / 32];
 ymm5 = a[u256 (64 * off + 16) * 2 / 32];
 ymm6 = a[u256 (64 * off + 32) * 2 / 32];
 ymm7 = a[u256 (64 * off + 48) * 2 / 32];


 ymm12, ymm13, ymm14, ymm15 = reduce_1(ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm3, ymm4, ymm5, ymm6, ymm8, ymm9, ymm10, ymm11 = update_1(ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15);

 r[u256 (64 * off + 0) * 2 / 32] = ymm3;
 r[u256 (64 * off + 16) * 2 / 32] = ymm4;
 r[u256 (64 * off + 32) * 2 / 32] = ymm5;
 r[u256 (64 * off + 48) * 2 / 32] = ymm6;

 r[u256 (64 * off + 128) * 2 / 32] = ymm8;
 r[u256 (64 * off + 144) * 2 / 32] = ymm9;
 r[u256 (64 * off + 160) * 2 / 32] = ymm10;
 r[u256 (64 * off + 176) * 2 / 32] = ymm11;

 return r;
}

inline fn levels1t7(reg ptr u16[SABER_N] r, reg ptr u16[864] pdata, inline int off, reg u256 ymm0, reg u256 ymm1, reg u256 ymm2) -> reg ptr u16[SABER_N]
{
 reg u256 ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;


 ymm15 = pdata[u256 (_ZETAS + 64 * off + 32) * 2 / 32];

 ymm8 = r[u256 (128 * off + 64) * 2 / 32];
 ymm9 = r[u256 (128 * off + 80) * 2 / 32];
 ymm10 = r[u256 (128 * off + 96) * 2 / 32];
 ymm11 = r[u256 (128 * off + 112) * 2 / 32];

 ymm3 = pdata[u256 (_ZETAS + 64 * off + 48) * 2 / 32];


 ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15 = mul(ymm8, ymm9, ymm10, ymm11, ymm15, ymm15, ymm3, ymm3);

 ymm4 = r[u256 (128 * off + 0) * 2 / 32];
 ymm5 = r[u256 (128 * off + 16) * 2 / 32];
 ymm6 = r[u256 (128 * off + 32) * 2 / 32];
 ymm7 = r[u256 (128 * off + 48) * 2 / 32];


 ymm12, ymm13, ymm14, ymm15 = reduce_1(ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm3, ymm4, ymm5, ymm6, ymm8, ymm9, ymm10, ymm11 = update_1(ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15);



 ymm7, ymm10 = shuffle8(ymm5, ymm10, ymm7, ymm10);


 ymm5, ymm11 = shuffle8(ymm6, ymm11, ymm5, ymm11);

 ymm15 = pdata[u256 (_ZETAS + 64 * off + 64) * 2 / 32];
 ymm6 = pdata[u256 (_ZETAS + 64 * off + 80) * 2 / 32];


 ymm7, ymm10, ymm5, ymm11, ymm12, ymm13, ymm14, ymm15 = mul(ymm7, ymm10, ymm5, ymm11, ymm15, ymm15, ymm6, ymm6);


 ymm6, ymm8 = shuffle8(ymm3, ymm8, ymm6, ymm8);


 ymm3, ymm9 = shuffle8(ymm4, ymm9, ymm3, ymm9);


 ymm12, ymm13, ymm14, ymm15 = reduce_1(ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm4, ymm6, ymm8, ymm3, ymm7, ymm10, ymm5, ymm11 = update_1(ymm4, ymm6, ymm8, ymm3, ymm9, ymm7, ymm10, ymm5, ymm11, ymm12, ymm13, ymm14, ymm15);



 ymm9, ymm7 = shuffle4(ymm4, ymm7, ymm9, ymm7);


 ymm4, ymm10 = shuffle4(ymm6, ymm10, ymm4, ymm10);


 ymm6, ymm5 = shuffle4(ymm8, ymm5, ymm6, ymm5);


 ymm8, ymm11 = shuffle4(ymm3, ymm11, ymm8, ymm11);

 ymm12 = #VPMULL_16u16(ymm9, pdata[u256 (_TWIST32 + 256 * off + 0) * 2 / 32]);
 ymm13 = #VPMULL_16u16(ymm7, pdata[u256 (_TWIST32 + 256 * off + 32) * 2 / 32]);
 ymm14 = #VPMULL_16u16(ymm4, pdata[u256 (_TWIST32 + 256 * off + 64) * 2 / 32]);
 ymm15 = #VPMULL_16u16(ymm10, pdata[u256 (_TWIST32 + 256 * off + 96) * 2 / 32]);

 ymm9 = #VPMULH_16u16(ymm9, pdata[u256 (_TWIST32 + 256 * off + 16) * 2 / 32]);
 ymm7 = #VPMULH_16u16(ymm7, pdata[u256 (_TWIST32 + 256 * off + 48) * 2 / 32]);
 ymm4 = #VPMULH_16u16(ymm4, pdata[u256 (_TWIST32 + 256 * off + 80) * 2 / 32]);
 ymm10 = #VPMULH_16u16(ymm10, pdata[u256 (_TWIST32 + 256 * off + 112) * 2 / 32]);


 ymm9, ymm7, ymm4, ymm10, ymm12, ymm13, ymm14, ymm15 = reduce_0(ymm9, ymm7, ymm4, ymm10, ymm0, ymm12, ymm13, ymm14, ymm15);

 ymm12 = #VPMULL_16u16(ymm6, pdata[u256 (_TWIST32 + 256 * off + 128) * 2 / 32]);
 ymm13 = #VPMULL_16u16(ymm5, pdata[u256 (_TWIST32 + 256 * off + 160) * 2 / 32]);
 ymm14 = #VPMULL_16u16(ymm8, pdata[u256 (_TWIST32 + 256 * off + 192) * 2 / 32]);
 ymm15 = #VPMULL_16u16(ymm11, pdata[u256 (_TWIST32 + 256 * off + 224) * 2 / 32]);

 ymm6 = #VPMULH_16u16(ymm6, pdata[u256 (_TWIST32 + 256 * off + 144) * 2 / 32]);
 ymm5 = #VPMULH_16u16(ymm5, pdata[u256 (_TWIST32 + 256 * off + 176) * 2 / 32]);
 ymm8 = #VPMULH_16u16(ymm8, pdata[u256 (_TWIST32 + 256 * off + 208) * 2 / 32]);
 ymm11 = #VPMULH_16u16(ymm11, pdata[u256 (_TWIST32 + 256 * off + 240) * 2 / 32]);


 ymm6, ymm5, ymm8, ymm11, ymm12, ymm13, ymm14, ymm15 = reduce_0(ymm6, ymm5, ymm8, ymm11, ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm3, ymm9, ymm7, ymm4, ymm6, ymm5, ymm8, ymm11 = update_0(ymm3, ymm9, ymm7, ymm4, ymm10, ymm6, ymm5, ymm8, ymm11);


 ymm14 = pdata[u256 (_16XMONT_PINV) * 2 / 32];
 ymm15 = pdata[u256 (_16XMONT) * 2 / 32];


 ymm7, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm7, ymm13, ymm0);


 ymm4, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm4, ymm13, ymm0);

 ymm12 = #VPMULL_16u16(ymm8, ymm1);
 ymm13 = #VPMULL_16u16(ymm11, ymm1);

 ymm8 = #VPMULH_16u16(ymm8, ymm2);
 ymm11 = #VPMULH_16u16(ymm11, ymm2);

 ymm10 = ymm3 +16u16 ymm7;
 ymm7 = ymm3 -16u16 ymm7;

 ymm3 = ymm9 +16u16 ymm4;
 ymm4 = ymm9 -16u16 ymm4;

 ymm12 = #VPMULH_16u16(ymm12, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);

 ymm9 = ymm6 +16u16 ymm8;
 ymm8 = ymm6 -16u16 ymm8;

 ymm6 = ymm5 +16u16 ymm11;
 ymm11 = ymm5 -16u16 ymm11;

 ymm9 = ymm9 -16u16 ymm12;
 ymm8 = ymm8 +16u16 ymm12;

 ymm6 = ymm6 -16u16 ymm13;
 ymm11 = ymm11 +16u16 ymm13;



 ymm3, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm3, ymm13, ymm0);

 ymm12 = #VPMULL_16u16(ymm4, ymm1);
 ymm13 = #VPMULL_16u16(ymm6, pdata[u256 (_ZETAS + 32) * 2 / 32]);
 ymm14 = #VPMULL_16u16(ymm11, pdata[u256 (_ZETAS + 96) * 2 / 32]);

 ymm4 = #VPMULH_16u16(ymm4, ymm2);
 ymm6 = #VPMULH_16u16(ymm6, pdata[u256 (_ZETAS + 48) * 2 / 32]);
 ymm11 = #VPMULH_16u16(ymm11, pdata[u256 (_ZETAS + 112) * 2 / 32]);

 ymm5 = ymm10 +16u16 ymm3;
 ymm3 = ymm10 -16u16 ymm3;

 ymm12 = #VPMULH_16u16(ymm12, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);
 ymm14 = #VPMULH_16u16(ymm14, ymm0);

 ymm10 = ymm7 +16u16 ymm4;
 ymm4 = ymm7 -16u16 ymm4;

 ymm7 = ymm9 +16u16 ymm6;
 ymm6 = ymm9 -16u16 ymm6;

 ymm9 = ymm8 +16u16 ymm11;
 ymm11 = ymm8 -16u16 ymm11;

 ymm10 = ymm10 -16u16 ymm12;
 ymm4 = ymm4 +16u16 ymm12;

 ymm7 = ymm7 -16u16 ymm13;
 ymm6 = ymm6 +16u16 ymm13;

 ymm9 = ymm9 -16u16 ymm14;
 ymm11 = ymm11 +16u16 ymm14;



 ymm5, ymm12 = fqmulprecomp1(_16XMONT_PINV, ymm5, ymm12, ymm0, pdata);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 8) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 12) * 2 / 8]);


 ymm3, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm3, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 16) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 20) * 2 / 8]);


 ymm10, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm10, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 24) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 28) * 2 / 8]);


 ymm4, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm4, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 32) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 36) * 2 / 8]);


 ymm7, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm7, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 40) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 44) * 2 / 8]);


 ymm6, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm6, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 48) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 52) * 2 / 8]);


 ymm9, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm9, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 56) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 60) * 2 / 8]);


 ymm11, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm11, ymm12, ymm0);


 ymm5, ymm8, ymm7 = shuffle2(ymm5, ymm7, ymm8, ymm7);


 ymm3, ymm5, ymm6 = shuffle2(ymm3, ymm6, ymm5, ymm6);


 ymm10, ymm3, ymm9 = shuffle2(ymm10, ymm9, ymm3, ymm9);


 ymm4, ymm10, ymm11 = shuffle2(ymm4, ymm11, ymm10, ymm11);


 ymm8, ymm4, ymm3 = shuffle1(ymm8, ymm3, ymm4, ymm3);


 ymm7, ymm8, ymm9 = shuffle1(ymm7, ymm9, ymm8, ymm9);


 ymm5, ymm7, ymm10 = shuffle1(ymm5, ymm10, ymm7, ymm10);


 ymm6, ymm5, ymm11 = shuffle1(ymm6, ymm11, ymm5, ymm11);


 ymm6, ymm4, ymm3, ymm7, ymm8, ymm9, ymm5, ymm11 = update_0(ymm6, ymm4, ymm3, ymm7, ymm10, ymm8, ymm9, ymm5, ymm11);


 ymm12 = #VPMULL_16u16(ymm9, ymm1);
 ymm13 = #VPMULL_16u16(ymm11, ymm1);

 ymm9 = #VPMULH_16u16(ymm9, ymm2);
 ymm11 = #VPMULH_16u16(ymm11, ymm2);

 ymm10 = ymm6 +16u16 ymm4;
 ymm4 = ymm6 -16u16 ymm4;

 ymm6 = ymm3 +16u16 ymm7;
 ymm7 = ymm3 -16u16 ymm7;

 ymm12 = #VPMULH_16u16(ymm12, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);

 ymm3 = ymm8 +16u16 ymm9;
 ymm9 = ymm8 -16u16 ymm9;

 ymm8 = ymm5 +16u16 ymm11;
 ymm11 = ymm5 -16u16 ymm11;

 ymm3 = ymm3 -16u16 ymm12;
 ymm9 = ymm9 +16u16 ymm12;

 ymm8 = ymm8 -16u16 ymm13;
 ymm11 = ymm11 +16u16 ymm13;

 r[u256 (128 * off + 0) * 2 / 32] = ymm10;
 r[u256 (128 * off + 16) * 2 / 32] = ymm4;
 r[u256 (128 * off + 32) * 2 / 32] = ymm3;
 r[u256 (128 * off + 48) * 2 / 32] = ymm9;
 r[u256 (128 * off + 64) * 2 / 32] = ymm6;
 r[u256 (128 * off + 80) * 2 / 32] = ymm7;
 r[u256 (128 * off + 96) * 2 / 32] = ymm8;
 r[u256 (128 * off + 112) * 2 / 32] = ymm11;

 return r;
}


fn poly_ntt_0(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0, ymm1, ymm2;

 ymm0 = PDATA0[u256 _16XP * 2 / 32];
 ymm1 = PDATA0[u256 (_ZETAS + 0) * 2 / 32];
 ymm2 = PDATA0[u256 (_ZETAS + 16) * 2 / 32];

 r = level0(r, a, 0, ymm0, ymm1, ymm2);
 r = level0(r, a, 1, ymm0, ymm1, ymm2);

 r = levels1t7(r, PDATA0, 0, ymm0, ymm1, ymm2);
 r = levels1t7(r, PDATA0, 1, ymm0, ymm1, ymm2);

 return r;
}



fn poly_ntt_1(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0, ymm1, ymm2;

 ymm0 = PDATA1[u256 _16XP * 2 / 32];
 ymm1 = PDATA1[u256 (_ZETAS + 0) * 2 / 32];
 ymm2 = PDATA1[u256 (_ZETAS + 16) * 2 / 32];

 r = level0(r, a, 0, ymm0, ymm1, ymm2);
 r = level0(r, a, 1, ymm0, ymm1, ymm2);

 r = levels1t7(r, PDATA1, 0, ymm0, ymm1, ymm2);
 r = levels1t7(r, PDATA1, 1, ymm0, ymm1, ymm2);

 return r;
}
inline fn polyvec_ntt_0(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_ntt_0(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N]);
 }

 return r;
}

inline fn polyvec_ntt_1(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_ntt_1(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N]);
 }

 return r;
}
inline fn pointwise32(reg ptr u16[SABER_N] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b, reg ptr u16[864] pdata, inline int off, reg u256 ymm0, reg u256 ymm1) -> reg ptr u16[SABER_N]
{
 reg u256 ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;

 ymm4 = a[u256 (0 + 32 * off + 0) * 2 / 32];
 ymm5 = b[u256 (0 + 32 * off + 0) * 2 / 32];

 ymm6 = a[u256 (0 + 32 * off + 16) * 2 / 32];
 ymm7 = b[u256 (0 + 32 * off + 16) * 2 / 32];

 ymm8 = a[u256 (256 + 32 * off + 0) * 2 / 32];
 ymm9 = b[u256 (256 + 32 * off + 0) * 2 / 32];

 ymm3 = #VPMULL_16u16(ymm4, ymm5);
 ymm4 = #VPMULH_16u16(ymm4, ymm5);

 ymm10 = a[u256 (256 + 32 * off + 16) * 2 / 32];
 ymm11 = b[u256 (256 + 32 * off + 16) * 2 / 32];

 ymm5 = #VPMULL_16u16(ymm6, ymm7);
 ymm6 = #VPMULH_16u16(ymm6, ymm7);

 ymm12 = a[u256 (512 + 32 * off + 0) * 2 / 32];
 ymm13 = b[u256 (512 + 32 * off + 0) * 2 / 32];

 ymm7 = #VPMULL_16u16(ymm8, ymm9);
 ymm8 = #VPMULH_16u16(ymm8, ymm9);

 ymm14 = a[u256 (512 + 32 * off + 16) * 2 / 32];
 ymm15 = b[u256 (512 + 32 * off + 16) * 2 / 32];

 ymm9 = #VPMULL_16u16(ymm10, ymm11);
 ymm10 = #VPMULH_16u16(ymm10, ymm11);

 ymm11 = #VPMULL_16u16(ymm12, ymm13);
 ymm12 = #VPMULH_16u16(ymm12, ymm13);

 ymm13 = #VPMULL_16u16(ymm14, ymm15);
 ymm14 = #VPMULH_16u16(ymm14, ymm15);

 ymm3 = #VPMULL_16u16(ymm3, ymm1);
 ymm5 = #VPMULL_16u16(ymm5, ymm1);
 ymm7 = #VPMULL_16u16(ymm7, ymm1);
 ymm9 = #VPMULL_16u16(ymm9, ymm1);
 ymm11 = #VPMULL_16u16(ymm11, ymm1);
 ymm13 = #VPMULL_16u16(ymm13, ymm1);

 ymm3 = #VPMULH_16u16(ymm3, ymm0);
 ymm5 = #VPMULH_16u16(ymm5, ymm0);
 ymm7 = #VPMULH_16u16(ymm7, ymm0);
 ymm9 = #VPMULH_16u16(ymm9, ymm0);
 ymm11 = #VPMULH_16u16(ymm11, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);

 ymm4 +16u16= ymm8;
 ymm6 +16u16= ymm10;
 ymm4 +16u16= ymm12;
 ymm6 +16u16= ymm14;

 ymm3 = ymm4 -16u16 ymm3;
 ymm4 = ymm6 -16u16 ymm5;
 ymm3 = ymm3 -16u16 ymm7;
 ymm4 = ymm4 -16u16 ymm9;
 ymm3 = ymm3 -16u16 ymm11;
 ymm4 = ymm4 -16u16 ymm13;







 ymm14 = pdata[u256 _16XF_PINV * 2 / 32];
 ymm15 = pdata[u256 _16XF * 2 / 32];


 ymm3, ymm5 = fqmulprecomp_0(ymm14, ymm15, ymm3, ymm5, ymm0);


 ymm4, ymm5 = fqmulprecomp_0(ymm14, ymm15, ymm4, ymm5, ymm0);

 r[u256 (32 * off + 0) * 2 / 32] = ymm3;
 r[u256 (32 * off + 16) * 2 / 32] = ymm4;

 return r;
}

fn polyvec_basemul_acc_montgomery_0(reg ptr u16[SABER_N] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0, ymm1, ymm2;

 ymm0 = PDATA0[u256 _16XP * 2 / 32];
 ymm1 = PDATA0[u256 _16XPINV * 2 / 32];

 r = pointwise32(r, a, b, PDATA0, 0, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 1, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 2, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 3, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 4, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 5, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 6, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 7, ymm0, ymm1);

 return r;
}

fn polyvec_basemul_acc_montgomery_1(reg ptr u16[SABER_N] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0, ymm1, ymm2;

 ymm0 = PDATA1[u256 _16XP * 2 / 32];
 ymm1 = PDATA1[u256 _16XPINV * 2 / 32];

 r = pointwise32(r, a, b, PDATA1, 0, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 1, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 2, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 3, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 4, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 5, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 6, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 7, ymm0, ymm1);

 return r;
}







inline fn butterfly(
reg u256 rl0, reg u256 rl1, reg u256 rl2, reg u256 rl3, reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3, reg u256 zl0, reg u256 zl1, reg u256 zh0, reg u256 zh1, reg u256 ymm0, reg u256 ymm12, reg u256 ymm13, reg u256 ymm14, reg u256 ymm15)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 ymm12 = rh0 -16u16 rl0;

 rl0 +16u16= rh0;
 ymm13 = rh1 -16u16 rl1;
 rh0 = #VPMULL_16u16(ymm12, zl0);

 rl1 +16u16= rh1;
 ymm14 = rh2 -16u16 rl2;
 rh1 = #VPMULL_16u16(ymm13, zl0);

 rl2 +16u16= rh2;
 ymm15 = rh3 -16u16 rl3;
 rh2 = #VPMULL_16u16(ymm14, zl1);

 rl3 +16u16= rh3;
 rh3 = #VPMULL_16u16(ymm15, zl1);

 ymm12 = #VPMULH_16u16(ymm12, zh0);
 ymm13 = #VPMULH_16u16(ymm13, zh0);

 ymm14 = #VPMULH_16u16(ymm14, zh1);
 ymm15 = #VPMULH_16u16(ymm15, zh1);

 rh0 = #VPMULH_16u16(rh0, ymm0);
 rh1 = #VPMULH_16u16(rh1, ymm0);
 rh2 = #VPMULH_16u16(rh2, ymm0);
 rh3 = #VPMULH_16u16(rh3, ymm0);

 rh0 = ymm12 -16u16 rh0;
 rh1 = ymm13 -16u16 rh1;
 rh2 = ymm14 -16u16 rh2;
 rh3 = ymm15 -16u16 rh3;

 return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3, ymm12, ymm13, ymm14, ymm15;
}

inline fn intt_levels0t6(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a, reg ptr u16[864] pdata, inline int off, reg u256 ymm0) -> reg ptr u16[SABER_N]
{
 reg u256 ymm1, ymm2, ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;


 ymm6 = a[u256 (128 * off + 32) * 2 / 32];
 ymm7 = a[u256 (128 * off + 48) * 2 / 32];
 ymm10 = a[u256 (128 * off + 96) * 2 / 32];
 ymm11 = a[u256 (128 * off + 112) * 2 / 32];

 ymm1 = pdata[u256 (_ZETAS + 0) * 2 /32];

 ymm12 = ymm7 -16u16 ymm6;
 ymm6 +16u16= ymm7;
 ymm7 = #VPMULL_16u16(ymm12, ymm1);

 ymm13 = ymm11 -16u16 ymm10;
 ymm10 +16u16= ymm11;
 ymm11 = #VPMULL_16u16(ymm13, ymm1);

 ymm4 = a[u256 (128 * off + 0) * 2 / 32];
 ymm5 = a[u256 (128 * off + 16) * 2 / 32];
 ymm8 = a[u256 (128 * off + 64) * 2 / 32];
 ymm9 = a[u256 (128 * off + 80) * 2 / 32];

 ymm2 = pdata[u256 (_ZETAS + 16) * 2 /32];

 ymm12 = #VPMULH_16u16(ymm12, ymm2);
 ymm13 = #VPMULH_16u16(ymm13, ymm2);
 ymm7 = #VPMULH_16u16(ymm7, ymm0);
 ymm11 = #VPMULH_16u16(ymm11, ymm0);

 ymm14 = ymm4 +16u16 ymm5;
 ymm5 = ymm4 -16u16 ymm5;

 ymm15 = ymm8 +16u16 ymm9;
 ymm9 = ymm8 -16u16 ymm9;

 ymm7 = ymm12 -16u16 ymm7;
 ymm11 = ymm13 -16u16 ymm11;



 ymm4 = ymm14 +16u16 ymm6;
 ymm6 = ymm14 -16u16 ymm6;

 ymm12 = ymm5 +16u16 ymm7;
 ymm7 = ymm5 -16u16 ymm7;

 ymm8 = ymm15 +16u16 ymm10;
 ymm10 = ymm15 -16u16 ymm10;

 ymm13 = ymm9 +16u16 ymm11;
 ymm11 = ymm9 -16u16 ymm11;



 ymm4, ymm14, ymm15 = shuffle1(ymm4, ymm12, ymm14, ymm15);


 ymm6, ymm5, ymm9 = shuffle1(ymm6, ymm7, ymm5, ymm9);


 ymm8, ymm6, ymm12 = shuffle1(ymm8, ymm13, ymm6, ymm12);


 ymm10, ymm7, ymm11 = shuffle1(ymm10, ymm11, ymm7, ymm11);


 ymm14, ymm4, ymm8 = shuffle2(ymm14, ymm5, ymm4, ymm8);


 ymm6, ymm5, ymm13 = shuffle2(ymm6, ymm7, ymm5, ymm13);


 ymm15, ymm6, ymm10 = shuffle2(ymm15, ymm9, ymm6, ymm10);


 ymm12, ymm7, ymm11 = shuffle2(ymm12, ymm11, ymm7, ymm11);


 ymm4, ymm14 = fqmulprecomp1(_16XMONT_PINV, ymm4, ymm14, ymm0, pdata);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 0) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 4) * 2 / 8]);


 ymm5, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm5, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 24) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 28) * 2 / 8]);


 ymm6, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm6, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 16) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 20) * 2 / 8]);


 ymm7, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm7, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 56) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 60) * 2 / 8]);


 ymm8, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm8, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 48) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 52) * 2 / 8]);


 ymm13, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm13, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 40) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 44) * 2 / 8]);


 ymm10, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm10, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 32) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 36) * 2 / 8]);


 ymm11, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm11, ymm14, ymm0);


 ymm12 = ymm7 -16u16 ymm6;
 ymm6 +16u16= ymm7;
 ymm7 = #VPMULL_16u16(ymm12, ymm1);

 ymm9 = ymm13 -16u16 ymm8;
 ymm8 +16u16= ymm13;
 ymm13 = #VPMULL_16u16(ymm9, pdata[u256 (_ZETAS + 96) * 2 / 32]);

 ymm14 = ymm11 -16u16 ymm10;
 ymm10 +16u16= ymm11;
 ymm11 = #VPMULL_16u16(ymm14, pdata[u256 (_ZETAS + 32) * 2 / 32]);

 ymm12 = #VPMULH_16u16(ymm12, ymm2);
 ymm9 = #VPMULH_16u16(ymm9, pdata[u256 (_ZETAS + 112) * 2 / 32]);
 ymm14 = #VPMULH_16u16(ymm14, pdata[u256 (_ZETAS + 48) * 2 / 32]);

 ymm7 = #VPMULH_16u16(ymm7, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);
 ymm11 = #VPMULH_16u16(ymm11, ymm0);

 ymm15 = ymm4 +16u16 ymm5;
 ymm5 = ymm4 -16u16 ymm5;

 ymm7 = ymm12 -16u16 ymm7;
 ymm9 -16u16= ymm13;
 ymm11 = ymm14 -16u16 ymm11;


 ymm12 = ymm10 -16u16 ymm8;
 ymm8 +16u16= ymm10;
 ymm10 = #VPMULL_16u16(ymm12, ymm1);

 ymm13 = ymm11 -16u16 ymm9;
 ymm9 +16u16= ymm11;
 ymm11 = #VPMULL_16u16(ymm13, ymm1);

 ymm12 = #VPMULH_16u16(ymm12, ymm2);
 ymm13 = #VPMULH_16u16(ymm13, ymm2);

 ymm10 = #VPMULH_16u16(ymm10, ymm0);
 ymm11 = #VPMULH_16u16(ymm11, ymm0);

 ymm4 = ymm15 +16u16 ymm6;
 ymm6 = ymm15 -16u16 ymm6;

 ymm15 = ymm5 +16u16 ymm7;
 ymm7 = ymm5 -16u16 ymm7;

 ymm10 = ymm12 -16u16 ymm10;
 ymm11 = ymm13 -16u16 ymm11;


 ymm13 = pdata[u256 (_16XMONT_PINV) * 2 / 32];
 ymm14 = pdata[u256 (_16XMONT) * 2 / 32];


 ymm4, ymm12 = fqmulprecomp_0(ymm13, ymm14, ymm4, ymm12, ymm0);




 ymm12 = ymm4 +16u16 ymm8;
 ymm8 = ymm4 -16u16 ymm8;

 ymm5 = ymm15 +16u16 ymm9;
 ymm9 = ymm15 -16u16 ymm9;

 ymm13 = ymm6 +16u16 ymm10;
 ymm10 = ymm6 -16u16 ymm10;

 ymm14 = ymm7 +16u16 ymm11;
 ymm11 = ymm7 -16u16 ymm11;

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 0) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 16) * 2 / 32], 0x1B);


 ymm12, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm12, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 32) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 48) * 2 / 32], 0x1B);


 ymm5, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm5, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 64) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 80) * 2 / 32], 0x1B);


 ymm13, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm13, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 96) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 112) * 2 / 32], 0x1B);


 ymm14, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm14, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 128) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 144) * 2 / 32], 0x1B);


 ymm8, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm8, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 160) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 176) * 2 / 32], 0x1B);


 ymm9, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm9, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 192) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 208) * 2 / 32], 0x1B);


 ymm10, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm10, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 224) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 240) * 2 / 32], 0x1B);


 ymm11, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm11, ymm7, ymm0);


 ymm3, ymm4 = shuffle4(ymm12, ymm5, ymm3, ymm4);


 ymm5, ymm6 = shuffle4(ymm13, ymm14, ymm3, ymm4);


 ymm7, ymm8 = shuffle4(ymm8, ymm9, ymm7, ymm8);


 ymm9, ymm10 = shuffle4(ymm10, ymm11, ymm9, ymm10);


 ymm2 = #VPERMQ(pdata[u256 (_ZETAS + (1 - off) * 64 + 64) * 2 / 32], 0x4E);
 ymm11 = #VPERMQ(pdata[u256 (_ZETAS + (1 - off) * 64 + 80) * 2 / 32], 0x4E);


 ymm3, ymm5, ymm7, ymm9, ymm4, ymm6, ymm8, ymm10, ymm12, ymm13, ymm14, ymm15 = butterfly(ymm3, ymm5, ymm7, ymm9, ymm4, ymm6, ymm8, ymm10, ymm2, ymm2, ymm11, ymm11, ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm11, ymm5 = shuffle8(ymm3, ymm5, ymm11, ymm5);


 ymm3, ymm9 = shuffle8(ymm7, ymm9, ymm3, ymm9);


 ymm7, ymm6 = shuffle8(ymm4, ymm6, ymm7, ymm6);


 ymm4, ymm10 = shuffle8(ymm8, ymm10, ymm4, ymm10);


 ymm2 = pdata[u256 (_ZETAS + (1 - off) * 64 + 32) * 2 / 32];
 ymm8 = pdata[u256 (_ZETAS + (1 - off) * 64 + 48) * 2 / 32];


 ymm11, ymm3, ymm7, ymm4, ymm5, ymm9, ymm6, ymm10, ymm12, ymm13, ymm14, ymm15 = butterfly(ymm11, ymm3, ymm7, ymm4, ymm5, ymm9, ymm6, ymm10, ymm2, ymm2, ymm8, ymm8, ymm0, ymm12, ymm13, ymm14, ymm15);

 ymm14 = pdata[u256 (_16XMONT_PINV) * 2 / 32];
 ymm15 = pdata[u256 (_16XMONT) * 2 / 32];


 ymm11, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm11, ymm13, ymm0);


 ymm3, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm3, ymm13, ymm0);

 r[u256 (128 * off + 0) * 2 / 32] = ymm11;
 r[u256 (128 * off + 16) * 2 / 32] = ymm3;
 r[u256 (128 * off + 32) * 2 / 32] = ymm7;
 r[u256 (128 * off + 48) * 2 / 32] = ymm4;
 r[u256 (128 * off + 64) * 2 / 32] = ymm5;
 r[u256 (128 * off + 80) * 2 / 32] = ymm9;
 r[u256 (128 * off + 96) * 2 / 32] = ymm6;
 r[u256 (128 * off + 112) * 2 / 32] = ymm10;

 return r;
}

inline fn intt_level7(reg ptr u16[SABER_N] r, reg ptr u16[864] pdata, inline int off, reg u256 ymm0) -> reg ptr u16[SABER_N]
{
 reg u256 ymm1, ymm2, ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;

 ymm4 = r[u256 (64 * off + 0) * 2 / 32];
 ymm8 = r[u256 (64 * off + 128) * 2 / 32];
 ymm5 = r[u256 (64 * off + 16) * 2 / 32];
 ymm9 = r[u256 (64 * off + 144) * 2 / 32];

 ymm1 = pdata[u256 (_ZETAS + 0) * 2 / 32];

 ymm6 = r[u256 (64 * off + 32) * 2 / 32];
 ymm10 = r[u256 (64 * off + 160) * 2 / 32];
 ymm7 = r[u256 (64 * off + 48) * 2 / 32];
 ymm11 = r[u256 (64 * off + 176) * 2 / 32];

 ymm2 = pdata[u256 (_ZETAS + 16) * 2 / 32];


 ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15 = butterfly(ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm1, ymm1, ymm2, ymm2, ymm0, ymm12, ymm13, ymm14, ymm15);

 r[u256 (64 * off + 0) * 2 / 32] = ymm4;
 r[u256 (64 * off + 16) * 2 / 32] = ymm5;
 r[u256 (64 * off + 32) * 2 / 32] = ymm6;
 r[u256 (64 * off + 48) * 2 / 32] = ymm7;
 r[u256 (64 * off + 128) * 2 / 32] = ymm8;
 r[u256 (64 * off + 144) * 2 / 32] = ymm9;
 r[u256 (64 * off + 160) * 2 / 32] = ymm10;
 r[u256 (64 * off + 176) * 2 / 32] = ymm11;

 return r;
}


fn poly_invntt_tomont_0(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0;

 ymm0 = PDATA0[u256 _16XP * 2 / 32];

 r = intt_levels0t6(r, a, PDATA0, 0, ymm0);
 r = intt_levels0t6(r, a, PDATA0, 1, ymm0);

 r = intt_level7(r, PDATA0, 0, ymm0);
 r = intt_level7(r, PDATA0, 1, ymm0);

 return r;
}



fn poly_invntt_tomont_1(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0;

 ymm0 = PDATA1[u256 _16XP * 2 / 32];

 r = intt_levels0t6(r, a, PDATA1, 0, ymm0);
 r = intt_levels0t6(r, a, PDATA1, 1, ymm0);

 r = intt_level7(r, PDATA1, 0, ymm0);
 r = intt_level7(r, PDATA1, 1, ymm0);

 return r;
}

inline fn polyvec_invntt_tomont_0(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_invntt_tomont_0(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N]);
 }

 return r;
}

inline fn polyvec_invntt_tomont_1(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_invntt_tomont_1(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N]);
 }

 return r;
}







inline fn mulmod(reg u256 a, reg u256 b_pinv, reg u256 b, reg u256 p) -> reg u256 {
 reg u256 t;
 reg u256 u;

 t = #VPMULL_16u16(a, b_pinv);
 u = #VPMULH_16u16(a, b);
 t = #VPMULH_16u16(t, p);
 t = u -16u16 t;

 return t;
}

fn poly_crt(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a, reg ptr u16[SABER_N] b) -> reg ptr u16[SABER_N]
{
 inline int i;

 reg u256 f0, f1;
 reg u256 u, u_pinv;
 reg u256 p0, p1;
 reg u256 mod;
 reg u256 mont0, mont0_pinv;

 u_pinv = #VPBROADCAST_16u16(CRT_U_PINV);
 u = #VPBROADCAST_16u16(CRT_U);
 p0 = PDATA0[u256 _16XP / 16];
 p1 = PDATA1[u256 _16XP / 16];
 mod = modq_16u16;
 mont0_pinv = PDATA0[u256 _16XMONT_PINV / 16];
 mont0 = PDATA0[u256 _16XMONT / 16];

 for i = 0 to SABER_N / 16 {
  f0 = a[u256 i];
  f1 = b[u256 i];
  f0 = mulmod(f0, mont0_pinv, mont0, p0);
  f1 -16u16= f0;
  f1 = mulmod(f1, u_pinv, u, p1);
  f1 = #VPMULL_16u16(f1, p0);
  f0 +16u16= f1;
  f0 &= mod;
  r[u256 i] = f0;
 }

 return r;
}

inline fn polyvec_crt(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_crt(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N], b[i * SABER_N:SABER_N]);
 }

 return r;
}

fn polyvec_matrix_vector_mul(reg ptr u16[SABER_KN] t, reg ptr u16[SABER_KKN] a, reg ptr u16[SABER_KN] s, reg u64 transpose) -> reg ptr u16[SABER_KN]
{
 inline int i;
 inline int j;

 stack u16[SABER_KN] ahat;
 stack u16[SABER_KN] shat;
 stack u16[SABER_KN] t0, t1;
 stack u16[SABER_KN] t00, t11;

 shat = polyvec_ntt_0(shat, s);
 for i = 0 to SABER_K {
  for j = 0 to SABER_K {
   if (transpose != 0) {
    ahat[j * SABER_N:SABER_N] = poly_ntt_0(ahat[j * SABER_N:SABER_N], a[j * SABER_KN + i * SABER_N:SABER_N]);
   } else {
    ahat[j * SABER_N:SABER_N] = poly_ntt_0(ahat[j * SABER_N:SABER_N], a[i * SABER_KN + j * SABER_N:SABER_N]);
   }
  }
  t00[i * SABER_N:SABER_N] = polyvec_basemul_acc_montgomery_0(t00[i * SABER_N:SABER_N], ahat, shat);
 }

 shat = polyvec_ntt_1(shat, s);
 for i = 0 to SABER_K {
  for j = 0 to SABER_K {
   if (transpose != 0) {
    ahat[j * SABER_N:SABER_N] = poly_ntt_1(ahat[j * SABER_N:SABER_N], a[j * SABER_KN + i * SABER_N:SABER_N]);
   } else {
    ahat[j * SABER_N:SABER_N] = poly_ntt_1(ahat[j * SABER_N:SABER_N], a[i * SABER_KN + j * SABER_N:SABER_N]);
   }
  }
  t11[i * SABER_N:SABER_N] = polyvec_basemul_acc_montgomery_1(t11[i * SABER_N:SABER_N], ahat, shat);
 }

 t0 = polyvec_invntt_tomont_0(t0, t00);
 t1 = polyvec_invntt_tomont_1(t1, t11);

 t = polyvec_crt(t, t0, t1);

 return t;
}

inline fn indcpa_kem_keypair_randominc(stack u8[SABER_INDCPA_PUBLICKEYBYTES] pk, stack u8[SABER_INDCPA_SECRETKEYBYTES] sk, stack u8[SABER_SEEDBYTES] seed, stack u8[SABER_COINBYTES] noiseseed) -> stack u8[SABER_INDCPA_PUBLICKEYBYTES], stack u8[SABER_INDCPA_SECRETKEYBYTES]
{
 inline int i;

 reg u8 t8;
 reg u64 transpose;

 stack u16[SABER_KKN] a;
 stack u16[SABER_KN] skpv;
 stack u16[SABER_KN] res;


 seed = shake128_32_32(seed, seed);

 a = GenMatrix(a, seed);

 skpv = GenSecret(skpv, noiseseed);

 for i = 0 to SABER_KN {
  res[i] = 0;
 }


 transpose = 1;
 res = polyvec_matrix_vector_mul(res, a, skpv, transpose);

 for i = 0 to SABER_KN {
  res[i] += h1;
  res[i] &= (SABER_Q - 1);
  res[i] >>= (SABER_EQ - SABER_EP);
 }

 sk = POLVECq2BS(sk, skpv);

 pk[0:SABER_POLYVECCOMPRESSEDBYTES] = POLVECp2BS(pk[0:SABER_POLYVECCOMPRESSEDBYTES], res);

 for i = 0 to SABER_SEEDBYTES {
  t8 = seed[i];
  pk[SABER_POLYVECCOMPRESSEDBYTES + i] = t8;
 }

 return pk, sk;
}







fn BS2POLVECp(reg ptr u8[SABER_POLYVECCOMPRESSEDBYTES] bytes, reg ptr u16[SABER_KN] data) -> reg ptr u16[SABER_KN]
{
 inline int i;
 inline int j;

 reg u16 b1;
 reg u16 b2;

 for i = 0 to SABER_K {
  for j = 0 to (SABER_N / 4) {

   b1 = (16u) bytes[i * (SABER_N * 10) / 8 + 5 * j];
   b2 = (16u) bytes[i * (SABER_N * 10) / 8 + 5 * j + 1];
   b1 &= 0xff;
   b2 &= 0x03;
   b2 <<= 8;
   b1 |= b2;
   data[i * SABER_N + 4 * j] = b1;


   b1 = (16u) bytes[i * (SABER_N * 10) / 8 + 5 * j + 1];
   b2 = (16u) bytes[i * (SABER_N * 10) / 8 + 5 * j + 2];
   b1 >>= 2;
   b1 &= 0x3f;
   b2 &= 0x0f;
   b2 <<= 6;
   b1 |= b2;
   data[i * SABER_N + 4 * j + 1] = b1;


   b1 = (16u) bytes[i * (SABER_N * 10) / 8 + 5 * j + 2];
   b2 = (16u) bytes[i * (SABER_N * 10) / 8 + 5 * j + 3];
   b1 >>= 4;
   b1 &= 0x0f;
   b2 &= 0x3f;
   b2 <<= 4;
   b1 |= b2;
   data[i * SABER_N + 4 * j + 2] = b1;


   b1 = (16u) bytes[i * (SABER_N * 10) / 8 + 5 * j + 3];
   b2 = (16u) bytes[i * (SABER_N * 10) / 8 + 5 * j + 4];
   b1 >>= 6;
   b1 &= 0x03;
   b2 &= 0xff;
   b2 <<= 2;
   b1 |= b2;
   data[i * SABER_N + 4 * j + 3] = b1;
  }
 }

 return data;
}







inline fn SABER_pack_4bit(reg ptr u8[SABER_SCALEBYTES_KEM] bytes, reg ptr u16[SABER_N] data) -> reg ptr u8[SABER_SCALEBYTES_KEM]
{
 inline int j;

 reg u16 d1;
 reg u16 d2;

 for j = 0 to (SABER_N / 2) {

  d1 = data[2 * j];
  d1 &= 0x0f;
  d2 = data[2* j + 1];
  d2 &= 0x0f;
  d2 <<= 4;
  d1 |= d2;
  bytes[j] = (8u) d1;
 }

 return bytes;
}



inline fn indcpa_kem_enc(stack u8[SABER_KEYBYTES] message_received, stack u8[32] noiseseed, stack u8[SABER_INDCPA_PUBLICKEYBYTES] pk, stack u8[SABER_BYTES_CCA_DEC] ciphertext) -> stack u8[SABER_BYTES_CCA_DEC]
{
 inline int i;
 inline int j;
 inline int k;

 reg u8 t8;

 reg u16 t16;

 stack u8[SABER_SEEDBYTES] seed;
 stack u8[SABER_SCALEBYTES_KEM] msk_c;

 stack u16[SABER_KKN] a;
 stack u16[SABER_KN] pkcl;
 stack u16[SABER_KN] skpv1;
 stack u16[SABER_KEYBYTES * 8] message;
 stack u16[SABER_KN] res;
 stack u16[SABER_N] vprime;


 for i = 0 to SABER_SEEDBYTES {
  t8 = pk[SABER_POLYVECCOMPRESSEDBYTES + i];
  seed[i] = t8;
 }

 a = GenMatrix(a, seed);

 skpv1 = GenSecret(skpv1, noiseseed);

 for i = 0 to SABER_KN {
  res[i] = 0;
 }

 res = MatrixVectorMul(a, skpv1, res, 0);

 for i = 0 to SABER_KN {
  res[i] += h1;
  res[i] &= (SABER_Q - 1);
  res[i] >>= (SABER_EQ - SABER_EP);
 }

 ciphertext[0:SABER_POLYVECCOMPRESSEDBYTES] = POLVECp2BS(ciphertext[0:SABER_POLYVECCOMPRESSEDBYTES], res);

 pkcl = BS2POLVECp(pk[0:SABER_POLYVECCOMPRESSEDBYTES], pkcl);

 for i = 0 to SABER_N {
  vprime[i] = 0;
 }

 for i = 0 to SABER_KN {
  skpv1[i] &= (SABER_P - 1);
 }

 vprime = InnerProd(pkcl, skpv1, vprime);

 for i = 0 to SABER_N {
  vprime[i] += h1;
 }

 for j = 0 to SABER_KEYBYTES {
  for i = 0 to 8 {

   t16 = (16u) message_received[j];
   t16 >>= i;
   t16 &= 0x01;
   message[8 * j + i] = t16;
  }
 }

 for i = 0 to SABER_N {

  message[i] <<= (SABER_EP - 1);
 }

 for k = 0 to SABER_N {

  t16 = vprime[k];
  t16 -= message[k];
  t16 &= (SABER_P - 1);
  t16 >>= (SABER_EP-SABER_ET);
  vprime[k] = t16;
 }

 msk_c = SABER_pack_4bit(msk_c, vprime);

 for j = 0 to SABER_SCALEBYTES_KEM {
  t8 = msk_c[j];
  ciphertext[SABER_POLYVECCOMPRESSEDBYTES + j] = t8;
 }

 return ciphertext;
}














inline fn BS2POLVECq(reg ptr u8[SABER_POLYVECBYTES] bytes, reg ptr u16[SABER_KN] data) -> reg ptr u16[SABER_KN]
{
 inline int i;
 inline int j;

 reg u16 b1;
 reg u16 b2;
 reg u16 b3;

 for i = 0 to SABER_K {
  for j = 0 to (SABER_N / 8) {

   b1 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j];
   b1 &= 0xff;
   b2 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 1];
   b2 &= 0x1f;
   b2 <<= 8;
   b1 |= b2;
   data[i * SABER_N + 8 * j] = b1;


   b1 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 1];
   b2 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 2];
   b3 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 3];
   b1 >>= 5;
   b1 &= 0x07;
   b2 &= 0xff;
   b2 <<= 3;
   b3 &= 0x03;
   b3 <<= 11;
   b2 |= b3;
   b1 |= b2;
   data[i * SABER_N + 8 * j + 1] = b1;


   b1 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 3];
   b2 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 4];
   b1 >>= 2;
   b1 &= 0x3f;
   b2 &= 0x7f;
   b2 <<= 6;
   b1 |= b2;
   data[i * SABER_N + 8 * j + 2] = b1;


   b1 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 4];
   b2 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 5];
   b3 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 6];
   b1 >>= 7;
   b1 &= 0x01;
   b2 &= 0xff;
   b2 <<= 1;
   b3 &= 0x0f;
   b3 <<= 9;
   b2 |= b3;
   b1 |= b2;
   data[i * SABER_N + 8 * j + 3] = b1;


   b1 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 6];
   b2 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 7];
   b3 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 8];
   b1 >>= 4;
   b1 &= 0x0f;
   b2 &= 0xff;
   b2 <<= 4;
   b3 &= 0x01;
   b3 <<= 12;
   b2 |= b3;
   b1 |= b2;
   data[i * SABER_N + 8 * j + 4] = b1;


   b1 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 8];
   b2 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 9];
   b1 >>= 1;
   b1 &= 0x7f;
   b2 &= 0x3f;
   b2 <<= 7;
   b1 |= b2;
   data[i * SABER_N + 8 * j + 5] = b1;


   b1 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 9];
   b2 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 10];
   b3 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 11];
   b1 >>= 6;
   b1 &= 0x03;
   b2 &= 0xff;
   b2 <<= 2;
   b3 &= 0x07;
   b3 <<= 10;
   b2 |= b3;
   b1 |= b2;
   data[i * SABER_N + 8 * j + 6] = b1;


   b1 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 11];
   b2 = (16u) bytes[i * (SABER_N * 13) / 8 + 13 * j + 12];
   b1 >>= 3;
   b1 &= 0x1f;
   b2 &= 0xff;
   b2 <<= 5;
   b1 |= b2;
   data[i * SABER_N + 8 * j + 7] = b1;
  }
 }

 return data;
}







inline fn SABER_un_pack4bit(reg ptr u8[SABER_SCALEBYTES_KEM] bytes, reg ptr u16[SABER_N] ar) -> reg ptr u16[SABER_N]
{
 inline int j;

 reg u16 b1;

 for j = 0 to (SABER_N / 2) {

  b1 = (16u) bytes[j];
  b1 &= 0x0f;
  ar[2 * j] = b1;


  b1 = (16u) bytes[j];
  b1 >>= 4;
  b1 &= 0x0f;
  ar[2 * j + 1] = b1;
 }

 return ar;
}

fn polyvec_iprod(reg ptr u16[SABER_N] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b) -> reg ptr u16[SABER_N]
{
 stack u16[SABER_N] t0, t1;
 stack u16[SABER_N] r0, r1;
 stack u16[SABER_KN] ahat;
 stack u16[SABER_KN] bhat;

 ahat = polyvec_ntt_0(ahat, a);
 bhat = polyvec_ntt_0(bhat, b);
 t0 = polyvec_basemul_acc_montgomery_0(t0, ahat, bhat);

 bhat = polyvec_ntt_1(bhat, b);
 ahat = polyvec_ntt_1(ahat, a);
 t1 = polyvec_basemul_acc_montgomery_1(t1, ahat, bhat);

 r0 = poly_invntt_tomont_0(r0, t0);
 r1 = poly_invntt_tomont_1(r1, t1);

 r = poly_crt(r, r0, r1);

 return r;
}

inline fn indcpa_kem_dec(stack u8[SABER_INDCPA_SECRETKEYBYTES] sk, stack u8[SABER_BYTES_CCA_DEC] ciphertext, stack u8[SABER_KEYBYTES] message_dec) -> stack u8[SABER_KEYBYTES]
{
 inline int i;

 reg u8 t8;

 reg u16 t16;

 stack u8[SABER_SCALEBYTES_KEM] scale_ar;

 stack u16[SABER_KN] sksv;
 stack u16[SABER_KN] pksv;
 stack u16[SABER_N] v;
 stack u16[SABER_N] op;

 sksv = BS2POLVECq(sk, sksv);

 pksv = BS2POLVECp(ciphertext[0:SABER_POLYVECCOMPRESSEDBYTES], pksv);

 for i = 0 to SABER_N {
  v[i] = 0;
 }

 for i = 0 to SABER_KN {
  sksv[i] &= (SABER_P - 1);
 }


 v = polyvec_iprod(v, pksv, sksv);

 for i = 0 to SABER_SCALEBYTES_KEM {
  t8 = ciphertext[SABER_POLYVECCOMPRESSEDBYTES + i];
  scale_ar[i] = t8;
 }

 op = SABER_un_pack4bit(scale_ar, op);

 for i = 0 to SABER_N {

  t16 = op[i];
  t16 <<= (SABER_EP-SABER_ET);
  v[i] += h2;
  v[i] -= t16;
  v[i] &= (SABER_P - 1);
  v[i] >>= (SABER_EP - 1);
 }

 message_dec = POL2MSG(v, message_dec);

 return message_dec;
}

export fn MatrixVectorMul_jazz(reg u64 ap, reg u64 skpvp, reg u64 resp, reg u16 transpose)
{
 inline int i;

 reg u16 t;

 stack u16[SABER_KKN] a;
 stack u16[SABER_KN] skpv;
 stack u16[SABER_KN] res;

 stack u64 sresp;

 for i = 0 to SABER_KKN {
  t = (u16) [ap + 2 * i];
  a[i] = t;
 }

 for i = 0 to SABER_KN {
  t = (u16) [skpvp + 2 * i];
  skpv[i] = t;
 }

 for i = 0 to SABER_KN {
  t = (u16) [resp + 2 * i];
  res[i] = t;
 }

 sresp = resp;

 res = MatrixVectorMul(a, skpv, res, transpose);

 resp = sresp;

 for i = 0 to SABER_KN {
  t = res[i];
  (u16) [resp + 2 * i] = t;
 }

}

export fn InnerProd_jazz(reg u64 pkclp, reg u64 skpvp, reg u64 resp)
{
 inline int i;

 reg u16 t;

 stack u16[SABER_KN] pkcl;
 stack u16[SABER_KN] skpv;
 stack u16[SABER_N] res;

 stack u64 sresp;

 for i = 0 to SABER_KN {
  t = (u16) [pkclp + 2 * i];
  pkcl[i] = t;
 }

 for i = 0 to SABER_KN {
  t = (u16) [skpvp + 2 * i];
  skpv[i] = t;
 }

 for i = 0 to SABER_N {
  t = (u16) [resp + 2 * i];
  res[i] = t;
 }

 sresp = resp;

 res = InnerProd(pkcl, skpv, res);

 resp = sresp;

 for i = 0 to SABER_KN {
  t = res[i];
  (u16) [resp + 2 * i] = t;
 }
}

export fn POL2MSG_jazz(reg u64 message_dec_unpackedp, reg u64 message_decp)
{
 inline int i;

 reg u8 tmd;

 reg u16 tmdu;

 stack u8[SABER_KEYBYTES] message_dec;
 stack u16[SABER_N] message_dec_unpacked;

 for i = 0 to SABER_N {
  tmdu = (u16) [message_dec_unpackedp + 2 * i];
  message_dec_unpacked[i] = tmdu;
 }

 message_dec = POL2MSG(message_dec_unpacked, message_dec);

 for i = 0 to SABER_KEYBYTES {
  tmd = message_dec[i];
  (u8) [message_decp + i] = tmd;
 }
}

export fn GenMatrix_jazz(reg u64 ap, reg u64 seedp)
{
 inline int i;

 reg u8 t8;

 reg u16 t16;

 stack u8[SABER_SEEDBYTES] seed;

 stack u16[SABER_KKN] a;

 stack u64 sap;

 for i = 0 to SABER_SEEDBYTES {
  t8 = (u8) [seedp + i];
  seed[i] = t8;
 }

 sap = ap;

 a = GenMatrix(a, seed);

 ap = sap;

 for i = 0 to SABER_KKN {
  t16 = a[i];
  (u16) [ap + 2 * i] = t16;
 }
}

export fn indcpa_kem_keypair_randominc_jazz(reg u64 pkp, reg u64 skp, reg u64 seedp, reg u64 noiseseedp)
{
 inline int i;

 reg u8 t8;

 stack u8[SABER_INDCPA_PUBLICKEYBYTES] pk;
 stack u8[SABER_INDCPA_SECRETKEYBYTES] sk;
 stack u8[SABER_SEEDBYTES] seed;
 stack u8[SABER_COINBYTES] noiseseed;

 stack u64 spkp;
 stack u64 sskp;

 for i = 0 to SABER_SEEDBYTES {
  t8 = (u8) [seedp + i];
  seed[i] = t8;
 }

 for i = 0 to SABER_COINBYTES {
  t8 = (u8) [noiseseedp + i];
  noiseseed[i] = t8;
 }

 spkp = pkp;
 sskp = skp;

 pk, sk = indcpa_kem_keypair_randominc(pk, sk, seed, noiseseed);

 pkp = spkp;
 skp = sskp;

 for i = 0 to SABER_INDCPA_PUBLICKEYBYTES {
  t8 = pk[i];
  (u8) [pkp + i] = t8;
 }

 for i = 0 to SABER_INDCPA_SECRETKEYBYTES {
  t8 = sk[i];
  (u8) [skp + i] = t8;
 }
}


export fn indcpa_kem_enc_jazz(reg u64 message_receivedp, reg u64 noiseseedp, reg u64 pkp, reg u64 ciphertextp)
{
 inline int i;

 reg u8 t8;

 stack u8[SABER_KEYBYTES] message_received;
 stack u8[SABER_COINBYTES] noiseseed;
 stack u8[SABER_INDCPA_PUBLICKEYBYTES] pk;
 stack u8[SABER_BYTES_CCA_DEC] ciphertext;

 stack u64 sciphertextp;

 for i = 0 to SABER_KEYBYTES {
  t8 = (u8) [message_receivedp + i];
  message_received[i] = t8;
 }

 for i = 0 to SABER_COINBYTES {
  t8 = (u8) [noiseseedp + i];
  noiseseed[i] = t8;
 }

 for i = 0 to SABER_INDCPA_PUBLICKEYBYTES {
  t8 = (u8) [pkp + i];
  pk[i] = t8;
 }

 sciphertextp = ciphertextp;

 ciphertext = indcpa_kem_enc(message_received, noiseseed, pk, ciphertext);

 ciphertextp = sciphertextp;

 for i = 0 to SABER_BYTES_CCA_DEC {
  t8 = ciphertext[i];
  (u8) [ciphertextp + i] = t8;
 }
}

export fn indcpa_kem_dec_jazz(reg u64 skp, reg u64 ciphertextp, reg u64 message_decp)
{
 inline int i;

 reg u8 t8;

 stack u8[SABER_INDCPA_SECRETKEYBYTES] sk;
 stack u8[SABER_BYTES_CCA_DEC] ciphertext;
 stack u8[SABER_KEYBYTES] message_dec;

 stack u64 smessage_decp;

 for i = 0 to SABER_INDCPA_SECRETKEYBYTES {
  t8 = (u8) [skp + i];
  sk[i] = t8;
 }

 for i = 0 to SABER_BYTES_CCA_DEC {
  t8 = (u8) [ciphertextp + i];
  ciphertext[i] = t8;
 }

 smessage_decp = message_decp;

 message_dec = indcpa_kem_dec(sk, ciphertext, message_dec);

 message_decp = smessage_decp;

 for i = 0 to SABER_KEYBYTES {
  t8 = message_dec[i];
  (u8) [message_decp + i] = t8;
 }
}
