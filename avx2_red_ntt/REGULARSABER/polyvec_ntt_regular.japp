








param int CRYPTO_SECRETKEYBYTES = 2304;
param int CRYPTO_PUBLICKEYBYTES = (3 * 320 + 32);
param int CRYPTO_BYTES = 32;
param int CRYPTO_CIPHERTEXTBYTES = 1088;
param int Saber_type = 2;

param int SABER_K = 3;
param int SABER_MU = 8;
param int SABER_ET = 4;

param int SABER_EQ = 13;
param int SABER_EP = 10;

param int SABER_N = 256;
param int SABER_Q = 8192;
param int SABER_P = 1024;

param int SABER_SEEDBYTES = 32;
param int SABER_NOISESEEDBYTES = 32;
param int SABER_COINBYTES = 32;
param int SABER_KEYBYTES = 32;

param int SABER_HASHBYTES = 32;

param int SABER_POLYBYTES = 416;

param int SABER_POLYVECBYTES = (SABER_K * SABER_POLYBYTES);

param int SABER_POLYVECCOMPRESSEDBYTES = (SABER_K * 320);

param int SABER_CIPHERTEXTBYTES = (SABER_POLYVECCOMPRESSEDBYTES);



param int SABER_SCALEBYTES_KEM = (SABER_ET * SABER_N / 8);

param int SABER_INDCPA_PUBLICKEYBYTES = (SABER_POLYVECCOMPRESSEDBYTES + SABER_SEEDBYTES);
param int SABER_INDCPA_SECRETKEYBYTES = (SABER_POLYVECBYTES);

param int SABER_PUBLICKEYBYTES = (SABER_INDCPA_PUBLICKEYBYTES);

param int SABER_SECRETKEYBYTES = (SABER_INDCPA_SECRETKEYBYTES + SABER_INDCPA_PUBLICKEYBYTES + SABER_HASHBYTES + SABER_KEYBYTES);

param int SABER_BYTES_CCA_DEC = (SABER_POLYVECCOMPRESSEDBYTES + SABER_SCALEBYTES_KEM);



param int SABER_KN = (SABER_K * SABER_N);
param int SABER_KKN = (SABER_K * SABER_K * SABER_N);
param int N_SB = (SABER_N / 4);
param int N_SB_RES = (2 * N_SB - 1);

param int SHAKE128_RATE = 168;
param int SHAKE256_RATE = 136;
param int SHA3_256_RATE = 136;
param int SHA3_512_RATE = 72;

param int KK13N8 = (SABER_K * SABER_K * (13 * SABER_N / 8));
param int MUNK8 = (SABER_MU * SABER_N * SABER_K / 8);

param int h1 = 4;
param int h2 = 228;




u128 zero_u128 = 0;

u256 zero_u256 = 0;

u256 h1_16u16 = 0x0004000400040004000400040004000400040004000400040004000400040004;
u256 h2_16u16 = 0x00e400e400e400e400e400e400e400e400e400e400e400e400e400e400e400e4;
u256 modp_16u16 = 0x03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff03ff;
u256 modq_16u16 = 0x1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff1fff;

u256 twobit_mask_16u16 = 0x0003000300030003000300030003000300030003000300030003000300030003;
u256 fourbit_mask_16u16 = 0x000f000f000f000f000f000f000f000f000f000f000f000f000f000f000f000f;
u256 sixbit_mask_16u16 = 0x003f003f003f003f003f003f003f003f003f003f003f003f003f003f003f003f;

u256 modq_8u32 = 0x00001fff00001fff00001fff00001fff00001fff00001fff00001fff00001fff;

u256 fourbit_mask_8u32 = 0x0000000f0000000f0000000f0000000f0000000f0000000f0000000f0000000f;
u256 sixteenbit_mask_8u32 = 0x0000ffff0000ffff0000ffff0000ffff0000ffff0000ffff0000ffff0000ffff;

u256 onebit_mask_64u4 = 0x1111111111111111111111111111111111111111111111111111111111111111;

u256 five_mask_64u4 = 0x5555555555555555555555555555555555555555555555555555555555555555;
u256 three_mask_64u4 = 0x3333333333333333333333333333333333333333333333333333333333333333;
u256 fourbit_mask_32u8 = 0x0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f;











param int _16XP = 0;
param int _16XPINV = 16;
param int _16XV = 32;
param int _16XSHIFT = 48;
param int _16XMONT_PINV = 64;
param int _16XMONT = 80;
param int _16XF_PINV = 96;
param int _16XF = 112;
param int _ZETAS = 128;
param int _TWIST32 = 288;
param int _TWIST4 = 800;


param int _REVIDXW = 0;
param int _REVIDXD = 32;
param int _SIGNMSKW = 64;

u16 P0 = 7681;
u16 P1 = 10753;
u16 CRT_U_PINV = 32747;
u16 CRT_U = 3563;


param int P_0 = 7681;
param int PINV_0 = -7679;
param int MONT_0 = -3593;
param int MONT_PINV_0 = -9;
param int V_0 = 17474;
param int SHIFT_0 = 16;
param int F_0 = 1912;
param int F_PINV_0 = -2184;

u16[864] PDATA0 = {

 P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0, P_0,


 PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0,
 PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0, PINV_0,


 V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0, V_0,


 SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0,
 SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0, SHIFT_0,


 MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0,
 MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0, MONT_PINV_0,


 MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0,
 MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0, MONT_0,


 F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0,
 F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0, F_PINV_0,


 F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0, F_0,


   28865, 28865, 28865, 28865, 28865, 28865, 28865, 28865,
   28865, 28865, 28865, 28865, 28865, 28865, 28865, 28865,
    3777, 3777, 3777, 3777, 3777, 3777, 3777, 3777,
    3777, 3777, 3777, 3777, 3777, 3777, 3777, 3777,
  -10350, -10350, -10350, -10350, -10350, -10350, -10350, -10350,
  -10350, -10350, -10350, -10350, -10350, -10350, -10350, -10350,
   -3182, -3182, -3182, -3182, -3182, -3182, -3182, -3182,
   -3182, -3182, -3182, -3182, -3182, -3182, -3182, -3182,
    4496, 4496, 4496, 4496, 4496, 4496, 4496, 4496,
   -7244, -7244, -7244, -7244, -7244, -7244, -7244, -7244,
   -3696, -3696, -3696, -3696, -3696, -3696, -3696, -3696,
   -1100, -1100, -1100, -1100, -1100, -1100, -1100, -1100,
   16425, 16425, 16425, 16425, 16425, 16425, 16425, 16425,
   16425, 16425, 16425, 16425, 16425, 16425, 16425, 16425,
    3625, 3625, 3625, 3625, 3625, 3625, 3625, 3625,
    3625, 3625, 3625, 3625, 3625, 3625, 3625, 3625,
   14744, 14744, 14744, 14744, 14744, 14744, 14744, 14744,
   -4974, -4974, -4974, -4974, -4974, -4974, -4974, -4974,
    2456, 2456, 2456, 2456, 2456, 2456, 2456, 2456,
    2194, 2194, 2194, 2194, 2194, 2194, 2194, 2194,


      -9, -529, 32738, -1851, -9, 29394, -7508, -20435,
      -9, 26288, 9855, -19215, -9, 16006, -12611, -964,
   -3593, -17, -1054, 3781, -3593, 3794, 2732, -2515,
   -3593, 1712, 2175, -3343, -3593, -3450, -2883, 1084,
   16279, 26288, -8558, -6297, 11783, -25648, 14351, -25733,
   21066, -23882, -17440, -7304, -26279, 16791, 22124, -20435,
   -3689, 1712, -1390, -1689, 7, -1072, -1521, 1403,
    -438, -2378, -1056, -3208, 1881, -3177, -404, -2515,
    2816, -22039, 9855, 21168, -19394, 30255, -27132, 17013,
   23489, -18506, 1869, 10145, -3114, 9650, -15358, -24232,
    2816, -2071, 2175, -3408, -1986, -2001, 3588, -1931,
   -1599, 2998, 3405, 1441, 2006, 434, 2, -3752,
    1724, -24214, 6032, -19215, -21467, 29453, -16655, 32124,
    4505, 13686, -25946, -12790, -23668, -31518, 14351, 12449,
    3772, 3434, -2160, -3343, 549, -1779, -783, 1404,
    -103, 2422, 3750, -1526, 2956, 226, -1521, 3745,
  -11655, -1715, 24743, 26766, 23754, 22943, -2722, 4880,
   18242, 26621, -32329, -10333, -22593, -16715, 30426, 2858,
     121, -179, -3417, 3214, 2250, -1121, -1698, -3312,
     834, 3581, -3145, -3677, 2495, -2891, 730, -2262,
   21066, -4624, -24573, -16186, 29667, -30597, 23225, 10333,
  -15998, 6510, -3558, 17491, 11792, 30255, -4693, 21723,
    -438, 3568, -1533, -2874, 3555, -3461, 2233, 3677,
    -638, -658, -486, -429, 3600, -2001, -2133, -293,
  -20469, -23882, 26663, 14718, -9488, -16885, -26220, 17636,
  -19351, -17082, 2722, 2807, 10972, -5990, 29871, -5299,
   -1525, -2378, -1497, -642, -1296, 2059, -3692, -796,
     617, -3770, 1698, -777, -3364, -2918, -2385, -3763,
   -4983, 18745, -17440, -32695, -4505, -12261, -32252, 23933,
    2073, -30938, 30136, 16083, -21467, -32414, -8908, -947,
   -1399, -2247, -1056, 3657, 103, -1509, -1532, 893,
   -2535, -1242, 1464, -1837, 549, -670, -2764, 589,
      -9, -1851, -8558, -22039, -9, 4573, -26441, 16791,
      -9, -6297, 6032, -4624, -9, -9513, 9360, 16006,
   -3593, 3781, -1390, -2071, -3593, -2083, 2743, -3177,
   -3593, -1689, -2160, 3568, -3593, 3287, 1168, -3450,
    1724, -19215, 24743, -4624, -21766, 1007, -15358, -25648,
   -4983, -7304, -16092, -13711, 21399, 4573, -12611, 29394,
    3772, -3343, -3417, 3568, -2310, 1519, 2, -1072,
   -1399, -3208, -1756, 2161, 1431, -2083, -2883, 3794,
  -20469, 14718, -17440, 16638, -15307, 12449, 12269, -22764,
  -26382, -5452, -25946, -11996, 5759, -964, -26441, 9087,
   -1525, -642, -1056, 1278, -1483, 3745, -2579, -236,
   -2830, 692, 3750, 2340, -1921, 1084, 2743, 1407,
    5930, -23933, -16092, -18506, 11792, -28805, -27132, -5990,
   -5913, 27243, -13933, 6510, -26279, -6766, -7508, 16791,
     810, -893, -1756, 2998, 3600, -1669, 3588, -2918,
   -1305, -2965, 915, -658, 1881, 402, 2732, -3177,
  -18191, -15221, -26262, 2739, -828, -15145, -8908, -9633,
   20315, -15111, -10478, 802, -20870, -4565, 22124, 26783,
   -2319, 3723, 1386, 1203, -2876, -2345, -2764, -929,
   -1701, -3335, -3310, -222, -1414, -2005, -404, 2719,
    4505, -5452, -3456, -28958, -14121, 32124, 17602, 2526,
    2073, 22790, -24052, 9633, -21766, -20435, 21868, 3524,
    -103, 692, -3456, 2786, -1321, 1404, 194, 3550,
   -2535, 3334, 2572, 929, -2310, -2515, -660, 1476,
    7491, -12790, -22875, 16885, 22568, 27858, 10478, 20119,
   31177, 5299, -21860, -10495, -3114, 1007, 8472, 9650,
   -2237, -1526, -859, -2059, 2088, 2258, 3310, 151,
    1993, 3763, -3428, -2815, 2006, 1519, -3816, 434,
   -5913, 27636, -32329, -2952, 29667, 23984, -10409, 8831,
  -11792, 14138, 13541, 31518, 11783, 30844, -15358, -19274,
   -1305, 1012, -3145, 1144, 3555, -592, 2391, 1151,
   -3600, 826, 2789, -226, 7, 124, 2, 2230,


      -9, -16425, -28865, 10350, -3593, -3625, -3777, 3182,
      -9, -10350, 28865, 16425, -3593, -3182, 3777, 3625,
      -9, 4496, -10350, 14744, -3593, -3696, -3182, 2456,
      -9, 4974, -16425, 7244, -3593, -2194, -3625, 1100,
      -9, -11655, 4496, -18191, -3593, 121, -3696, -2319,
      -9, -22593, 7244, -20315, -3593, 2495, 1100, 1701,
      -9, -18191, 14744, -23754, -3593, -2319, 2456, -2250,
      -9, -20870, 4974, -22593, -3593, -1414, -2194, 2495
};



param int P_1 = 10753;
param int MONT_1 = 1018;
param int MONT_PINV_1 = -6;
param int PINV_1 = -10751;
param int V_1 = 12482;
param int SHIFT_1 = 16;
param int F_1 = 2536;
param int F_PINV_1 = -1560;


u16[864] PDATA1 = {

 P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1, P_1,


 PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1,
 PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1, PINV_1,


 V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1, V_1,


 SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1,
 SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1, SHIFT_1,


 MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1,
 MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1, MONT_PINV_1,


 MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1,
 MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1, MONT_1,


 F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1,
 F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1, F_PINV_1,


 F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1, F_1,


   27359, 27359, 27359, 27359, 27359, 27359, 27359, 27359,
   27359, 27359, 27359, 27359, 27359, 27359, 27359, 27359,
     223, 223, 223, 223, 223, 223, 223, 223,
     223, 223, 223, 223, 223, 223, 223, 223,
   -1956, -1956, -1956, -1956, -1956, -1956, -1956, -1956,
   -1956, -1956, -1956, -1956, -1956, -1956, -1956, -1956,
    4188, 4188, 4188, 4188, 4188, 4188, 4188, 4188,
    4188, 4188, 4188, 4188, 4188, 4188, 4188, 4188,
   10093, 10093, 10093, 10093, 10093, 10093, 10093, 10093,
  -21094, -21094, -21094, -21094, -21094, -21094, -21094, -21094,
    2413, 2413, 2413, 2413, 2413, 2413, 2413, 2413,
   -3686, -3686, -3686, -3686, -3686, -3686, -3686, -3686,
     408, 408, 408, 408, 408, 408, 408, 408,
     408, 408, 408, 408, 408, 408, 408, 408,
   -3688, -3688, -3688, -3688, -3688, -3688, -3688, -3688,
   -3688, -3688, -3688, -3688, -3688, -3688, -3688, -3688,
   28517, 28517, 28517, 28517, 28517, 28517, 28517, 28517,
  -20856, -20856, -20856, -20856, -20856, -20856, -20856, -20856,
     357, 357, 357, 357, 357, 357, 357, 357,
    -376, -376, -376, -376, -376, -376, -376, -376,


      -6, -61, -609, -6095, -6, 14237, -31235, 23836,
      -6, -19643, -2017, -13811, -6, 27329, 11300, -7722,
    1018, -573, 5023, -3535, 1018, -1635, 2045, -2788,
    1018, 1349, 3615, -5107, 1018, 5313, 5156, -554,
    4589, -19643, 177, 1767, 24098, 1725, -31418, -7801,
  -12378, 16236, 31558, 232, 22209, 29644, -18845, 23836,
   -3091, 1349, 2737, -4889, -3550, 2237, 326, 1927,
    2982, -2196, -2234, 4328, 193, -5172, -2973, -2788,
   17675, -19863, -2017, -20173, 4547, -4083, -29364, -21593,
   25543, 11123, 512, 11623, 7429, -21161, -11555, -24129,
    4875, -5015, 3615, 3891, 4035, 4621, 1356, 4519,
    2503, 2419, 512, 4967, -4347, -3241, 5341, -2113,
   -5126, 14280, 11726, -13811, -20490, 24025, -24037, -13024,
  -27152, -19564, -8801, 12415, -6381, -26286, -31418, -23952,
   -4102, 1992, -1586, -5107, 3062, -2087, 4123, 3360,
   -2576, -1132, -3169, 1663, 1299, 3410, 326, 624,
   -7033, -4797, 17571, -20899, 16090, 31583, 16614, -13164,
  -29449, -19454, 17096, -16809, -12476, -26292, -4090, -12653,
    2695, -5309, 675, -4003, 730, 4447, -794, 5268,
    4855, 2050, 4808, 1111, -2236, 4428, -5114, -4973,
  -12378, 7289, 7356, 8027, 15864, -31467, -24976, 16809,
   22532, 6747, -13012, 4967, -20198, -4083, 25555, -31497,
    2982, -2439, -2884, 3419, -4616, -2283, -400, -1111,
       4, 2139, 1324, -1689, -2790, 4621, 467, 2807,
   14731, 16236, 31290, -14780, -10001, 32351, -7795, -9691,
   18363, 5729, -16614, -4248, 3639, 3346, 4394, 22483,
    1931, -2196, -454, -4540, 3823, 5215, 909, -5083,
   -2629, 97, 794, -152, 5175, 274, -2774, -2605,
  -16724, 29370, 31558, -12098, 27152, 12336, 19844, -22215,
    5766, -29827, 7856, 23093, -20490, -3035, -21892, -8935,
   -2388, -2374, -2234, -834, 2576, 4144, -2684, 825,
    4742, 3453, -336, 3125, 3062, 1573, 636, -2279,
      -6, -6095, 177, -19863, -6, -18077, -7326, 29644,
      -6, 1767, 11726, 7289, -6, -19661, 11141, 27329,
    1018, -3535, 2737, -5015, 1018, -2205, -2206, -5172,
    1018, -4889, -1586, -2439, 1018, 4403, -635, 5313,
   -5126, -13811, 17571, 7289, -23781, -18918, -11555, 1725,
  -16724, 232, -1627, 13158, 15840, -18077, 11300, 14237,
   -4102, -5107, 675, -2439, 4379, -1510, 5341, 2237,
   -2388, 4328, 2981, -4250, -544, -2205, 5156, -1635,
   14731, -14780, 31558, -30144, 3925, -23952, -780, -20070,
  -14847, -19856, -8801, -3699, -11683, -7722, -7326, 25482,
    1931, -4540, -2234, 2624, 341, 624, 1268, -2662,
   -4095, 4720, -3169, 5005, 5213, -554, -2206, 1930,
    2316, 22215, -1627, 11123, -20198, -6594, -29364, 3346,
   24269, -25652, -31887, 6747, 22209, 15328, -31235, 29644,
     268, -825, 2981, 2419, -2790, 4670, 1356, 274,
     205, 5068, 3441, 2139, 193, -1056, 2045, -5172,
  -18345, 5120, 7716, -17394, 28224, 24165, -21892, 14329,
    9508, -4717, -8246, 32070, 16072, 8161, -18845, 24330,
    -425, 5120, 1572, 2062, -4544, -3995, 636, 4601,
    3364, 2963, 970, -1722, 3784, 2529, -2973, 778,
  -27152, -19856, 969, -13987, 31217, -13024, -29407, 7880,
    5766, 31924, -17352, -14329, -23781, 23836, 22044, 8758,
   -2576, 4720, -567, 2909, 1009, 3360, -2271, -4408,
    4742, 1204, -5064, -4601, 4379, -2788, -4580, -458,
  -28103, 12415, 28541, -32351, -23056, -30467, 8246, 12976,
   26518, -22483, 32076, 3998, 7429, -18918, -14999, -21161,
   -5063, 1663, -3715, -5215, 1520, 2813, -970, 4784,
     918, 2605, -2740, -1122, -4347, -1510, -151, -3241,
   24269, 20661, 17096, -9343, 15864, -951, -1932, -28712,
   20198, -24641, 2395, 26286, 24098, 15517, -11555, 11952,
     205, 693, 4808, 1409, -4616, -2487, 116, -40,
    2790, -2625, -2213, -3410, -3550, -355, 5341, 3760,


      -6, -408, -27359, 1956, 1018, 3688, -223, -4188,
      -6, -1956, 27359, 408, 1018, 4188, 223, -3688,
      -6, 10093, -1956, 28517, 1018, 2413, 4188, 357,
      -6, 20856, -408, 21094, 1018, 376, 3688, 3686,
      -6, -7033, 10093, -18345, 1018, 2695, 2413, -425,
      -6, -12476, 21094, -9508, 1018, -2236, 3686, -3364,
      -6, -18345, 28517, -16090, 1018, -425, 357, -730,
      -6, 16072, 20856, -12476, 1018, 3784, 376, -2236
};
inline fn mulmod(reg u256 a, reg u256 b_pinv, reg u256 b, reg u256 p) -> reg u256 {
 reg u256 t;
 reg u256 u;

 t = #VPMULL_16u16(a, b_pinv);
 u = #VPMULH_16u16(a, b);
 t = #VPMULH_16u16(t, p);
 t = u -16u16 t;

 return t;
}

fn poly_crt(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a, reg ptr u16[SABER_N] b) -> reg ptr u16[SABER_N]
{
 inline int i;

 reg u256 f0, f1;
 reg u256 u, u_pinv;
 reg u256 p0, p1;
 reg u256 mod;
 reg u256 mont0, mont0_pinv;

 u_pinv = #VPBROADCAST_16u16(CRT_U_PINV);
 u = #VPBROADCAST_16u16(CRT_U);
 p0 = PDATA0[u256 _16XP / 16];
 p1 = PDATA1[u256 _16XP / 16];
 mod = modq_16u16;
 mont0_pinv = PDATA0[u256 _16XMONT_PINV / 16];
 mont0 = PDATA0[u256 _16XMONT / 16];

 for i = 0 to SABER_N / 16 {
  f0 = a[u256 i];
  f1 = b[u256 i];
  f0 = mulmod(f0, mont0_pinv, mont0, p0);
  f1 -16u16= f0;
  f1 = mulmod(f1, u_pinv, u, p1);
  f1 = #VPMULL_16u16(f1, p0);
  f0 +16u16= f1;
  f0 &= mod;
  r[u256 i] = f0;
 }

 return r;
}

inline fn polyvec_crt(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_crt(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N], b[i * SABER_N:SABER_N]);
 }

 return r;
}



















inline fn shuffle8(reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3) -> reg u256, reg u256
{
 r2 = #VPERM2I128(r0, r1, 0x20);
 r3 = #VPERM2I128(r0, r1, 0x31);

 return r2, r3;
}





inline fn shuffle4(reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3) -> reg u256, reg u256
{
 r2 = #VPUNPCKL_4u64(r0, r1);
 r3 = #VPUNPCKH_4u64(r0, r1);

 return r2, r3;
}





inline fn shuffle2(reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3) -> reg u256, reg u256, reg u256
{
 r2 = #VMOVSLDUP_8u32(r1);
 r2 = #VPBLEND_8u32(r0, r2, 0xAA);
 r0 >>4u64= 32;
 r3 = #VPBLEND_8u32(r0, r1, 0xAA);

 return r0, r2, r3;
}





inline fn shuffle1(reg u256 r0, reg u256 r1, reg u256 r2, reg u256 r3) -> reg u256, reg u256, reg u256
{
 r2 = #VPSLL_8u32(r1, 16);
 r2 = #VPBLEND_16u16(r0, r2, 0xAA);
 r0 = #VPSRL_8u32(r0, 16);
 r3 = #VPBLEND_16u16(r0, r1, 0xAA);

 return r0, r2, r3;
}





inline fn fqmulprecomp_0(reg u256 al, reg u256 ah, reg u256 b, reg u256 x, reg u256 ymm0) -> reg u256, reg u256
{
 x = #VPMULL_16u16(b, al);
 b = #VPMULH_16u16(b, ah);
 x = #VPMULH_16u16(x, ymm0);

 b -16u16= x;

 return b, x;
}

inline fn fqmulprecomp_1(reg u256 al, reg u256 ah, reg u256 b, reg u256 x, reg u256 ymm0) -> reg u256, reg u256
{
 x = #VPMULL_16u16(b, al);
 b = #VPMULH_16u16(b, ah);
 x = #VPMULH_16u16(x, ymm0);

 b = x -16u16 b;

 return b, x;
}





inline fn fqmulprecomp1(inline int off, reg u256 b, reg u256 x, reg u256 ymm0, reg ptr u16[864] pdata) -> reg u256, reg u256
{
 x = #VPMULL_16u16(b, pdata[u256 off * 2 / 32]);
 b = #VPMULH_16u16(b, pdata[u256 (off + 16) * 2 / 32]);
 x = #VPMULH_16u16(x, ymm0);

 b -16u16= x;

 return b, x;
}


inline fn update_0(reg u256 rln, reg u256 rl0, reg u256 rl1, reg u256 rl2, reg u256 rl3, reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 rln = rl0 +16u16 rh0;
 rh0 = rl0 -16u16 rh0;

 rl0 = rl1 +16u16 rh1;
 rh1 = rl1 -16u16 rh1;

 rl1 = rl2 +16u16 rh2;
 rh2 = rl2 -16u16 rh2;

 rl2 = rl3 +16u16 rh3;
 rh3 = rl3 -16u16 rh3;

 return rln, rl0, rl1, rl2, rh0, rh1, rh2, rh3;
}


inline fn update_1(reg u256 rln, reg u256 rl0, reg u256 rl1, reg u256 rl2, reg u256 rl3, reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3, reg u256 ymm12, reg u256 ymm13, reg u256 ymm14, reg u256 ymm15)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 rln = rl0 +16u16 rh0;
 rh0 = rl0 -16u16 rh0;

 rl0 = rl1 +16u16 rh1;
 rh1 = rl1 -16u16 rh1;

 rl1 = rl2 +16u16 rh2;
 rh2 = rl2 -16u16 rh2;

 rl2 = rl3 +16u16 rh3;
 rh3 = rl3 -16u16 rh3;

 rln -16u16= ymm12;
 rh0 +16u16= ymm12;

 rl0 -16u16= ymm13;
 rh1 +16u16= ymm13;

 rl1 -16u16= ymm14;
 rh2 +16u16= ymm14;

 rl2 -16u16= ymm15;
 rh3 +16u16= ymm15;

 return rln, rl0, rl1, rl2, rh0, rh1, rh2, rh3;
}


inline fn reduce_0(reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3, reg u256 ymm0, reg u256 ymm12, reg u256 ymm13, reg u256 ymm14, reg u256 ymm15)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 ymm12 = #VPMULH_16u16(ymm0, ymm12);
 ymm13 = #VPMULH_16u16(ymm0, ymm13);

 ymm14 = #VPMULH_16u16(ymm0, ymm14);
 ymm15 = #VPMULH_16u16(ymm0, ymm15);

 rh0 -16u16= ymm12;
 rh1 -16u16= ymm13;
 rh2 -16u16= ymm14;
 rh3 -16u16= ymm15;

 return rh0, rh1, rh2, rh3, ymm12, ymm13, ymm14, ymm15;
}


inline fn reduce_1(reg u256 ymm0, reg u256 ymm12, reg u256 ymm13, reg u256 ymm14, reg u256 ymm15) -> reg u256, reg u256, reg u256, reg u256
{
 ymm12 = #VPMULH_16u16(ymm0, ymm12);
 ymm13 = #VPMULH_16u16(ymm0, ymm13);

 ymm14 = #VPMULH_16u16(ymm0, ymm14);
 ymm15 = #VPMULH_16u16(ymm0, ymm15);

 return ymm12, ymm13, ymm14, ymm15;
}

inline fn mul(reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3, reg u256 zl0, reg u256 zl1, reg u256 zh0, reg u256 zh1)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 reg u256 ymm12, ymm13, ymm14, ymm15;

 ymm12 = #VPMULL_16u16(zl0, rh0);
 ymm13 = #VPMULL_16u16(zl0, rh1);

 ymm14 = #VPMULL_16u16(zl1, rh2);
 ymm15 = #VPMULL_16u16(zl1, rh3);

 rh0 = #VPMULH_16u16(zh0, rh0);
 rh1 = #VPMULH_16u16(zh0, rh1);

 rh2 = #VPMULH_16u16(zh1, rh2);
 rh3 = #VPMULH_16u16(zh1, rh3);

 return rh0, rh1, rh2, rh3, ymm12, ymm13, ymm14, ymm15;
}

inline fn level0(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a, inline int off, reg u256 ymm0, reg u256 ymm1, reg u256 ymm2) -> reg ptr u16[SABER_N]
{
 reg u256 ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;

 ymm8 = a[u256 (64 * off + 128) * 2 / 32];
 ymm9 = a[u256 (64 * off + 144) * 2 / 32];
 ymm10 = a[u256 (64 * off + 160) * 2 / 32];
 ymm11 = a[u256 (64 * off + 176) * 2 / 32];


 ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15 = mul(ymm8, ymm9, ymm10, ymm11, ymm1, ymm1, ymm2, ymm2);

 ymm4 = a[u256 (64 * off + 0) * 2 / 32];
 ymm5 = a[u256 (64 * off + 16) * 2 / 32];
 ymm6 = a[u256 (64 * off + 32) * 2 / 32];
 ymm7 = a[u256 (64 * off + 48) * 2 / 32];


 ymm12, ymm13, ymm14, ymm15 = reduce_1(ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm3, ymm4, ymm5, ymm6, ymm8, ymm9, ymm10, ymm11 = update_1(ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15);

 r[u256 (64 * off + 0) * 2 / 32] = ymm3;
 r[u256 (64 * off + 16) * 2 / 32] = ymm4;
 r[u256 (64 * off + 32) * 2 / 32] = ymm5;
 r[u256 (64 * off + 48) * 2 / 32] = ymm6;

 r[u256 (64 * off + 128) * 2 / 32] = ymm8;
 r[u256 (64 * off + 144) * 2 / 32] = ymm9;
 r[u256 (64 * off + 160) * 2 / 32] = ymm10;
 r[u256 (64 * off + 176) * 2 / 32] = ymm11;

 return r;
}

inline fn levels1t7(reg ptr u16[SABER_N] r, reg ptr u16[864] pdata, inline int off, reg u256 ymm0, reg u256 ymm1, reg u256 ymm2) -> reg ptr u16[SABER_N]
{
 reg u256 ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;


 ymm15 = pdata[u256 (_ZETAS + 64 * off + 32) * 2 / 32];

 ymm8 = r[u256 (128 * off + 64) * 2 / 32];
 ymm9 = r[u256 (128 * off + 80) * 2 / 32];
 ymm10 = r[u256 (128 * off + 96) * 2 / 32];
 ymm11 = r[u256 (128 * off + 112) * 2 / 32];

 ymm3 = pdata[u256 (_ZETAS + 64 * off + 48) * 2 / 32];


 ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15 = mul(ymm8, ymm9, ymm10, ymm11, ymm15, ymm15, ymm3, ymm3);

 ymm4 = r[u256 (128 * off + 0) * 2 / 32];
 ymm5 = r[u256 (128 * off + 16) * 2 / 32];
 ymm6 = r[u256 (128 * off + 32) * 2 / 32];
 ymm7 = r[u256 (128 * off + 48) * 2 / 32];


 ymm12, ymm13, ymm14, ymm15 = reduce_1(ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm3, ymm4, ymm5, ymm6, ymm8, ymm9, ymm10, ymm11 = update_1(ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15);



 ymm7, ymm10 = shuffle8(ymm5, ymm10, ymm7, ymm10);


 ymm5, ymm11 = shuffle8(ymm6, ymm11, ymm5, ymm11);

 ymm15 = pdata[u256 (_ZETAS + 64 * off + 64) * 2 / 32];
 ymm6 = pdata[u256 (_ZETAS + 64 * off + 80) * 2 / 32];


 ymm7, ymm10, ymm5, ymm11, ymm12, ymm13, ymm14, ymm15 = mul(ymm7, ymm10, ymm5, ymm11, ymm15, ymm15, ymm6, ymm6);


 ymm6, ymm8 = shuffle8(ymm3, ymm8, ymm6, ymm8);


 ymm3, ymm9 = shuffle8(ymm4, ymm9, ymm3, ymm9);


 ymm12, ymm13, ymm14, ymm15 = reduce_1(ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm4, ymm6, ymm8, ymm3, ymm7, ymm10, ymm5, ymm11 = update_1(ymm4, ymm6, ymm8, ymm3, ymm9, ymm7, ymm10, ymm5, ymm11, ymm12, ymm13, ymm14, ymm15);



 ymm9, ymm7 = shuffle4(ymm4, ymm7, ymm9, ymm7);


 ymm4, ymm10 = shuffle4(ymm6, ymm10, ymm4, ymm10);


 ymm6, ymm5 = shuffle4(ymm8, ymm5, ymm6, ymm5);


 ymm8, ymm11 = shuffle4(ymm3, ymm11, ymm8, ymm11);

 ymm12 = #VPMULL_16u16(ymm9, pdata[u256 (_TWIST32 + 256 * off + 0) * 2 / 32]);
 ymm13 = #VPMULL_16u16(ymm7, pdata[u256 (_TWIST32 + 256 * off + 32) * 2 / 32]);
 ymm14 = #VPMULL_16u16(ymm4, pdata[u256 (_TWIST32 + 256 * off + 64) * 2 / 32]);
 ymm15 = #VPMULL_16u16(ymm10, pdata[u256 (_TWIST32 + 256 * off + 96) * 2 / 32]);

 ymm9 = #VPMULH_16u16(ymm9, pdata[u256 (_TWIST32 + 256 * off + 16) * 2 / 32]);
 ymm7 = #VPMULH_16u16(ymm7, pdata[u256 (_TWIST32 + 256 * off + 48) * 2 / 32]);
 ymm4 = #VPMULH_16u16(ymm4, pdata[u256 (_TWIST32 + 256 * off + 80) * 2 / 32]);
 ymm10 = #VPMULH_16u16(ymm10, pdata[u256 (_TWIST32 + 256 * off + 112) * 2 / 32]);


 ymm9, ymm7, ymm4, ymm10, ymm12, ymm13, ymm14, ymm15 = reduce_0(ymm9, ymm7, ymm4, ymm10, ymm0, ymm12, ymm13, ymm14, ymm15);

 ymm12 = #VPMULL_16u16(ymm6, pdata[u256 (_TWIST32 + 256 * off + 128) * 2 / 32]);
 ymm13 = #VPMULL_16u16(ymm5, pdata[u256 (_TWIST32 + 256 * off + 160) * 2 / 32]);
 ymm14 = #VPMULL_16u16(ymm8, pdata[u256 (_TWIST32 + 256 * off + 192) * 2 / 32]);
 ymm15 = #VPMULL_16u16(ymm11, pdata[u256 (_TWIST32 + 256 * off + 224) * 2 / 32]);

 ymm6 = #VPMULH_16u16(ymm6, pdata[u256 (_TWIST32 + 256 * off + 144) * 2 / 32]);
 ymm5 = #VPMULH_16u16(ymm5, pdata[u256 (_TWIST32 + 256 * off + 176) * 2 / 32]);
 ymm8 = #VPMULH_16u16(ymm8, pdata[u256 (_TWIST32 + 256 * off + 208) * 2 / 32]);
 ymm11 = #VPMULH_16u16(ymm11, pdata[u256 (_TWIST32 + 256 * off + 240) * 2 / 32]);


 ymm6, ymm5, ymm8, ymm11, ymm12, ymm13, ymm14, ymm15 = reduce_0(ymm6, ymm5, ymm8, ymm11, ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm3, ymm9, ymm7, ymm4, ymm6, ymm5, ymm8, ymm11 = update_0(ymm3, ymm9, ymm7, ymm4, ymm10, ymm6, ymm5, ymm8, ymm11);


 ymm14 = pdata[u256 (_16XMONT_PINV) * 2 / 32];
 ymm15 = pdata[u256 (_16XMONT) * 2 / 32];


 ymm7, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm7, ymm13, ymm0);


 ymm4, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm4, ymm13, ymm0);

 ymm12 = #VPMULL_16u16(ymm8, ymm1);
 ymm13 = #VPMULL_16u16(ymm11, ymm1);

 ymm8 = #VPMULH_16u16(ymm8, ymm2);
 ymm11 = #VPMULH_16u16(ymm11, ymm2);

 ymm10 = ymm3 +16u16 ymm7;
 ymm7 = ymm3 -16u16 ymm7;

 ymm3 = ymm9 +16u16 ymm4;
 ymm4 = ymm9 -16u16 ymm4;

 ymm12 = #VPMULH_16u16(ymm12, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);

 ymm9 = ymm6 +16u16 ymm8;
 ymm8 = ymm6 -16u16 ymm8;

 ymm6 = ymm5 +16u16 ymm11;
 ymm11 = ymm5 -16u16 ymm11;

 ymm9 = ymm9 -16u16 ymm12;
 ymm8 = ymm8 +16u16 ymm12;

 ymm6 = ymm6 -16u16 ymm13;
 ymm11 = ymm11 +16u16 ymm13;



 ymm3, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm3, ymm13, ymm0);

 ymm12 = #VPMULL_16u16(ymm4, ymm1);
 ymm13 = #VPMULL_16u16(ymm6, pdata[u256 (_ZETAS + 32) * 2 / 32]);
 ymm14 = #VPMULL_16u16(ymm11, pdata[u256 (_ZETAS + 96) * 2 / 32]);

 ymm4 = #VPMULH_16u16(ymm4, ymm2);
 ymm6 = #VPMULH_16u16(ymm6, pdata[u256 (_ZETAS + 48) * 2 / 32]);
 ymm11 = #VPMULH_16u16(ymm11, pdata[u256 (_ZETAS + 112) * 2 / 32]);

 ymm5 = ymm10 +16u16 ymm3;
 ymm3 = ymm10 -16u16 ymm3;

 ymm12 = #VPMULH_16u16(ymm12, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);
 ymm14 = #VPMULH_16u16(ymm14, ymm0);

 ymm10 = ymm7 +16u16 ymm4;
 ymm4 = ymm7 -16u16 ymm4;

 ymm7 = ymm9 +16u16 ymm6;
 ymm6 = ymm9 -16u16 ymm6;

 ymm9 = ymm8 +16u16 ymm11;
 ymm11 = ymm8 -16u16 ymm11;

 ymm10 = ymm10 -16u16 ymm12;
 ymm4 = ymm4 +16u16 ymm12;

 ymm7 = ymm7 -16u16 ymm13;
 ymm6 = ymm6 +16u16 ymm13;

 ymm9 = ymm9 -16u16 ymm14;
 ymm11 = ymm11 +16u16 ymm14;



 ymm5, ymm12 = fqmulprecomp1(_16XMONT_PINV, ymm5, ymm12, ymm0, pdata);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 8) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 12) * 2 / 8]);


 ymm3, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm3, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 16) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 20) * 2 / 8]);


 ymm10, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm10, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 24) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 28) * 2 / 8]);


 ymm4, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm4, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 32) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 36) * 2 / 8]);


 ymm7, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm7, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 40) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 44) * 2 / 8]);


 ymm6, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm6, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 48) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 52) * 2 / 8]);


 ymm9, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm9, ymm12, ymm0);

 ymm12 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 56) * 2 / 8]);
 ymm13 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 60) * 2 / 8]);


 ymm11, ymm12 = fqmulprecomp_0(ymm12, ymm13, ymm11, ymm12, ymm0);


 ymm5, ymm8, ymm7 = shuffle2(ymm5, ymm7, ymm8, ymm7);


 ymm3, ymm5, ymm6 = shuffle2(ymm3, ymm6, ymm5, ymm6);


 ymm10, ymm3, ymm9 = shuffle2(ymm10, ymm9, ymm3, ymm9);


 ymm4, ymm10, ymm11 = shuffle2(ymm4, ymm11, ymm10, ymm11);


 ymm8, ymm4, ymm3 = shuffle1(ymm8, ymm3, ymm4, ymm3);


 ymm7, ymm8, ymm9 = shuffle1(ymm7, ymm9, ymm8, ymm9);


 ymm5, ymm7, ymm10 = shuffle1(ymm5, ymm10, ymm7, ymm10);


 ymm6, ymm5, ymm11 = shuffle1(ymm6, ymm11, ymm5, ymm11);


 ymm6, ymm4, ymm3, ymm7, ymm8, ymm9, ymm5, ymm11 = update_0(ymm6, ymm4, ymm3, ymm7, ymm10, ymm8, ymm9, ymm5, ymm11);


 ymm12 = #VPMULL_16u16(ymm9, ymm1);
 ymm13 = #VPMULL_16u16(ymm11, ymm1);

 ymm9 = #VPMULH_16u16(ymm9, ymm2);
 ymm11 = #VPMULH_16u16(ymm11, ymm2);

 ymm10 = ymm6 +16u16 ymm4;
 ymm4 = ymm6 -16u16 ymm4;

 ymm6 = ymm3 +16u16 ymm7;
 ymm7 = ymm3 -16u16 ymm7;

 ymm12 = #VPMULH_16u16(ymm12, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);

 ymm3 = ymm8 +16u16 ymm9;
 ymm9 = ymm8 -16u16 ymm9;

 ymm8 = ymm5 +16u16 ymm11;
 ymm11 = ymm5 -16u16 ymm11;

 ymm3 = ymm3 -16u16 ymm12;
 ymm9 = ymm9 +16u16 ymm12;

 ymm8 = ymm8 -16u16 ymm13;
 ymm11 = ymm11 +16u16 ymm13;

 r[u256 (128 * off + 0) * 2 / 32] = ymm10;
 r[u256 (128 * off + 16) * 2 / 32] = ymm4;
 r[u256 (128 * off + 32) * 2 / 32] = ymm3;
 r[u256 (128 * off + 48) * 2 / 32] = ymm9;
 r[u256 (128 * off + 64) * 2 / 32] = ymm6;
 r[u256 (128 * off + 80) * 2 / 32] = ymm7;
 r[u256 (128 * off + 96) * 2 / 32] = ymm8;
 r[u256 (128 * off + 112) * 2 / 32] = ymm11;

 return r;
}


fn poly_ntt_0(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0, ymm1, ymm2;

 ymm0 = PDATA0[u256 _16XP * 2 / 32];
 ymm1 = PDATA0[u256 (_ZETAS + 0) * 2 / 32];
 ymm2 = PDATA0[u256 (_ZETAS + 16) * 2 / 32];

 r = level0(r, a, 0, ymm0, ymm1, ymm2);
 r = level0(r, a, 1, ymm0, ymm1, ymm2);

 r = levels1t7(r, PDATA0, 0, ymm0, ymm1, ymm2);
 r = levels1t7(r, PDATA0, 1, ymm0, ymm1, ymm2);

 return r;
}



fn poly_ntt_1(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0, ymm1, ymm2;

 ymm0 = PDATA1[u256 _16XP * 2 / 32];
 ymm1 = PDATA1[u256 (_ZETAS + 0) * 2 / 32];
 ymm2 = PDATA1[u256 (_ZETAS + 16) * 2 / 32];

 r = level0(r, a, 0, ymm0, ymm1, ymm2);
 r = level0(r, a, 1, ymm0, ymm1, ymm2);

 r = levels1t7(r, PDATA1, 0, ymm0, ymm1, ymm2);
 r = levels1t7(r, PDATA1, 1, ymm0, ymm1, ymm2);

 return r;
}

inline fn polyvec_ntt_0(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_ntt_0(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N]);
 }

 return r;
}

inline fn polyvec_ntt_1(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_ntt_1(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N]);
 }

 return r;
}







inline fn butterfly(
reg u256 rl0, reg u256 rl1, reg u256 rl2, reg u256 rl3, reg u256 rh0, reg u256 rh1, reg u256 rh2, reg u256 rh3, reg u256 zl0, reg u256 zl1, reg u256 zh0, reg u256 zh1, reg u256 ymm0, reg u256 ymm12, reg u256 ymm13, reg u256 ymm14, reg u256 ymm15)
-> reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256, reg u256
{
 ymm12 = rh0 -16u16 rl0;

 rl0 +16u16= rh0;
 ymm13 = rh1 -16u16 rl1;
 rh0 = #VPMULL_16u16(ymm12, zl0);

 rl1 +16u16= rh1;
 ymm14 = rh2 -16u16 rl2;
 rh1 = #VPMULL_16u16(ymm13, zl0);

 rl2 +16u16= rh2;
 ymm15 = rh3 -16u16 rl3;
 rh2 = #VPMULL_16u16(ymm14, zl1);

 rl3 +16u16= rh3;
 rh3 = #VPMULL_16u16(ymm15, zl1);

 ymm12 = #VPMULH_16u16(ymm12, zh0);
 ymm13 = #VPMULH_16u16(ymm13, zh0);

 ymm14 = #VPMULH_16u16(ymm14, zh1);
 ymm15 = #VPMULH_16u16(ymm15, zh1);

 rh0 = #VPMULH_16u16(rh0, ymm0);
 rh1 = #VPMULH_16u16(rh1, ymm0);
 rh2 = #VPMULH_16u16(rh2, ymm0);
 rh3 = #VPMULH_16u16(rh3, ymm0);

 rh0 = ymm12 -16u16 rh0;
 rh1 = ymm13 -16u16 rh1;
 rh2 = ymm14 -16u16 rh2;
 rh3 = ymm15 -16u16 rh3;

 return rl0, rl1, rl2, rl3, rh0, rh1, rh2, rh3, ymm12, ymm13, ymm14, ymm15;
}

inline fn intt_levels0t6(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a, reg ptr u16[864] pdata, inline int off, reg u256 ymm0) -> reg ptr u16[SABER_N]
{
 reg u256 ymm1, ymm2, ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;


 ymm6 = a[u256 (128 * off + 32) * 2 / 32];
 ymm7 = a[u256 (128 * off + 48) * 2 / 32];
 ymm10 = a[u256 (128 * off + 96) * 2 / 32];
 ymm11 = a[u256 (128 * off + 112) * 2 / 32];

 ymm1 = pdata[u256 (_ZETAS + 0) * 2 /32];

 ymm12 = ymm7 -16u16 ymm6;
 ymm6 +16u16= ymm7;
 ymm7 = #VPMULL_16u16(ymm12, ymm1);

 ymm13 = ymm11 -16u16 ymm10;
 ymm10 +16u16= ymm11;
 ymm11 = #VPMULL_16u16(ymm13, ymm1);

 ymm4 = a[u256 (128 * off + 0) * 2 / 32];
 ymm5 = a[u256 (128 * off + 16) * 2 / 32];
 ymm8 = a[u256 (128 * off + 64) * 2 / 32];
 ymm9 = a[u256 (128 * off + 80) * 2 / 32];

 ymm2 = pdata[u256 (_ZETAS + 16) * 2 /32];

 ymm12 = #VPMULH_16u16(ymm12, ymm2);
 ymm13 = #VPMULH_16u16(ymm13, ymm2);
 ymm7 = #VPMULH_16u16(ymm7, ymm0);
 ymm11 = #VPMULH_16u16(ymm11, ymm0);

 ymm14 = ymm4 +16u16 ymm5;
 ymm5 = ymm4 -16u16 ymm5;

 ymm15 = ymm8 +16u16 ymm9;
 ymm9 = ymm8 -16u16 ymm9;

 ymm7 = ymm12 -16u16 ymm7;
 ymm11 = ymm13 -16u16 ymm11;



 ymm4 = ymm14 +16u16 ymm6;
 ymm6 = ymm14 -16u16 ymm6;

 ymm12 = ymm5 +16u16 ymm7;
 ymm7 = ymm5 -16u16 ymm7;

 ymm8 = ymm15 +16u16 ymm10;
 ymm10 = ymm15 -16u16 ymm10;

 ymm13 = ymm9 +16u16 ymm11;
 ymm11 = ymm9 -16u16 ymm11;



 ymm4, ymm14, ymm15 = shuffle1(ymm4, ymm12, ymm14, ymm15);


 ymm6, ymm5, ymm9 = shuffle1(ymm6, ymm7, ymm5, ymm9);


 ymm8, ymm6, ymm12 = shuffle1(ymm8, ymm13, ymm6, ymm12);


 ymm10, ymm7, ymm11 = shuffle1(ymm10, ymm11, ymm7, ymm11);


 ymm14, ymm4, ymm8 = shuffle2(ymm14, ymm5, ymm4, ymm8);


 ymm6, ymm5, ymm13 = shuffle2(ymm6, ymm7, ymm5, ymm13);


 ymm15, ymm6, ymm10 = shuffle2(ymm15, ymm9, ymm6, ymm10);


 ymm12, ymm7, ymm11 = shuffle2(ymm12, ymm11, ymm7, ymm11);


 ymm4, ymm14 = fqmulprecomp1(_16XMONT_PINV, ymm4, ymm14, ymm0, pdata);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 0) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 4) * 2 / 8]);


 ymm5, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm5, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 24) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 28) * 2 / 8]);


 ymm6, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm6, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 16) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 20) * 2 / 8]);


 ymm7, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm7, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 56) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 60) * 2 / 8]);


 ymm8, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm8, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 48) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 52) * 2 / 8]);


 ymm13, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm13, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 40) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 44) * 2 / 8]);


 ymm10, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm10, ymm14, ymm0);

 ymm14 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 32) * 2 / 8]);
 ymm15 = #VPBROADCAST_4u64(pdata[u64 (_TWIST4 + 36) * 2 / 8]);


 ymm11, ymm14 = fqmulprecomp_0(ymm14, ymm15, ymm11, ymm14, ymm0);


 ymm12 = ymm7 -16u16 ymm6;
 ymm6 +16u16= ymm7;
 ymm7 = #VPMULL_16u16(ymm12, ymm1);

 ymm9 = ymm13 -16u16 ymm8;
 ymm8 +16u16= ymm13;
 ymm13 = #VPMULL_16u16(ymm9, pdata[u256 (_ZETAS + 96) * 2 / 32]);

 ymm14 = ymm11 -16u16 ymm10;
 ymm10 +16u16= ymm11;
 ymm11 = #VPMULL_16u16(ymm14, pdata[u256 (_ZETAS + 32) * 2 / 32]);

 ymm12 = #VPMULH_16u16(ymm12, ymm2);
 ymm9 = #VPMULH_16u16(ymm9, pdata[u256 (_ZETAS + 112) * 2 / 32]);
 ymm14 = #VPMULH_16u16(ymm14, pdata[u256 (_ZETAS + 48) * 2 / 32]);

 ymm7 = #VPMULH_16u16(ymm7, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);
 ymm11 = #VPMULH_16u16(ymm11, ymm0);

 ymm15 = ymm4 +16u16 ymm5;
 ymm5 = ymm4 -16u16 ymm5;

 ymm7 = ymm12 -16u16 ymm7;
 ymm9 -16u16= ymm13;
 ymm11 = ymm14 -16u16 ymm11;


 ymm12 = ymm10 -16u16 ymm8;
 ymm8 +16u16= ymm10;
 ymm10 = #VPMULL_16u16(ymm12, ymm1);

 ymm13 = ymm11 -16u16 ymm9;
 ymm9 +16u16= ymm11;
 ymm11 = #VPMULL_16u16(ymm13, ymm1);

 ymm12 = #VPMULH_16u16(ymm12, ymm2);
 ymm13 = #VPMULH_16u16(ymm13, ymm2);

 ymm10 = #VPMULH_16u16(ymm10, ymm0);
 ymm11 = #VPMULH_16u16(ymm11, ymm0);

 ymm4 = ymm15 +16u16 ymm6;
 ymm6 = ymm15 -16u16 ymm6;

 ymm15 = ymm5 +16u16 ymm7;
 ymm7 = ymm5 -16u16 ymm7;

 ymm10 = ymm12 -16u16 ymm10;
 ymm11 = ymm13 -16u16 ymm11;


 ymm13 = pdata[u256 (_16XMONT_PINV) * 2 / 32];
 ymm14 = pdata[u256 (_16XMONT) * 2 / 32];


 ymm4, ymm12 = fqmulprecomp_0(ymm13, ymm14, ymm4, ymm12, ymm0);




 ymm12 = ymm4 +16u16 ymm8;
 ymm8 = ymm4 -16u16 ymm8;

 ymm5 = ymm15 +16u16 ymm9;
 ymm9 = ymm15 -16u16 ymm9;

 ymm13 = ymm6 +16u16 ymm10;
 ymm10 = ymm6 -16u16 ymm10;

 ymm14 = ymm7 +16u16 ymm11;
 ymm11 = ymm7 -16u16 ymm11;

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 0) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 16) * 2 / 32], 0x1B);


 ymm12, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm12, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 32) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 48) * 2 / 32], 0x1B);


 ymm5, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm5, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 64) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 80) * 2 / 32], 0x1B);


 ymm13, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm13, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 96) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 112) * 2 / 32], 0x1B);


 ymm14, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm14, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 128) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 144) * 2 / 32], 0x1B);


 ymm8, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm8, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 160) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 176) * 2 / 32], 0x1B);


 ymm9, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm9, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 192) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 208) * 2 / 32], 0x1B);


 ymm10, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm10, ymm7, ymm0);

 ymm7 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 224) * 2 / 32], 0x1B);
 ymm15 = #VPERMQ(pdata[u256 (_TWIST32 + 256 * (1 - off) + 240) * 2 / 32], 0x1B);


 ymm11, ymm7 = fqmulprecomp_0(ymm7, ymm15, ymm11, ymm7, ymm0);


 ymm3, ymm4 = shuffle4(ymm12, ymm5, ymm3, ymm4);


 ymm5, ymm6 = shuffle4(ymm13, ymm14, ymm3, ymm4);


 ymm7, ymm8 = shuffle4(ymm8, ymm9, ymm7, ymm8);


 ymm9, ymm10 = shuffle4(ymm10, ymm11, ymm9, ymm10);


 ymm2 = #VPERMQ(pdata[u256 (_ZETAS + (1 - off) * 64 + 64) * 2 / 32], 0x4E);
 ymm11 = #VPERMQ(pdata[u256 (_ZETAS + (1 - off) * 64 + 80) * 2 / 32], 0x4E);


 ymm3, ymm5, ymm7, ymm9, ymm4, ymm6, ymm8, ymm10, ymm12, ymm13, ymm14, ymm15 = butterfly(ymm3, ymm5, ymm7, ymm9, ymm4, ymm6, ymm8, ymm10, ymm2, ymm2, ymm11, ymm11, ymm0, ymm12, ymm13, ymm14, ymm15);


 ymm11, ymm5 = shuffle8(ymm3, ymm5, ymm11, ymm5);


 ymm3, ymm9 = shuffle8(ymm7, ymm9, ymm3, ymm9);


 ymm7, ymm6 = shuffle8(ymm4, ymm6, ymm7, ymm6);


 ymm4, ymm10 = shuffle8(ymm8, ymm10, ymm4, ymm10);


 ymm2 = pdata[u256 (_ZETAS + (1 - off) * 64 + 32) * 2 / 32];
 ymm8 = pdata[u256 (_ZETAS + (1 - off) * 64 + 48) * 2 / 32];


 ymm11, ymm3, ymm7, ymm4, ymm5, ymm9, ymm6, ymm10, ymm12, ymm13, ymm14, ymm15 = butterfly(ymm11, ymm3, ymm7, ymm4, ymm5, ymm9, ymm6, ymm10, ymm2, ymm2, ymm8, ymm8, ymm0, ymm12, ymm13, ymm14, ymm15);

 ymm14 = pdata[u256 (_16XMONT_PINV) * 2 / 32];
 ymm15 = pdata[u256 (_16XMONT) * 2 / 32];


 ymm11, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm11, ymm13, ymm0);


 ymm3, ymm13 = fqmulprecomp_0(ymm14, ymm15, ymm3, ymm13, ymm0);

 r[u256 (128 * off + 0) * 2 / 32] = ymm11;
 r[u256 (128 * off + 16) * 2 / 32] = ymm3;
 r[u256 (128 * off + 32) * 2 / 32] = ymm7;
 r[u256 (128 * off + 48) * 2 / 32] = ymm4;
 r[u256 (128 * off + 64) * 2 / 32] = ymm5;
 r[u256 (128 * off + 80) * 2 / 32] = ymm9;
 r[u256 (128 * off + 96) * 2 / 32] = ymm6;
 r[u256 (128 * off + 112) * 2 / 32] = ymm10;

 return r;
}

inline fn intt_level7(reg ptr u16[SABER_N] r, reg ptr u16[864] pdata, inline int off, reg u256 ymm0) -> reg ptr u16[SABER_N]
{
 reg u256 ymm1, ymm2, ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;

 ymm4 = r[u256 (64 * off + 0) * 2 / 32];
 ymm8 = r[u256 (64 * off + 128) * 2 / 32];
 ymm5 = r[u256 (64 * off + 16) * 2 / 32];
 ymm9 = r[u256 (64 * off + 144) * 2 / 32];

 ymm1 = pdata[u256 (_ZETAS + 0) * 2 / 32];

 ymm6 = r[u256 (64 * off + 32) * 2 / 32];
 ymm10 = r[u256 (64 * off + 160) * 2 / 32];
 ymm7 = r[u256 (64 * off + 48) * 2 / 32];
 ymm11 = r[u256 (64 * off + 176) * 2 / 32];

 ymm2 = pdata[u256 (_ZETAS + 16) * 2 / 32];


 ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15 = butterfly(ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm1, ymm1, ymm2, ymm2, ymm0, ymm12, ymm13, ymm14, ymm15);

 r[u256 (64 * off + 0) * 2 / 32] = ymm4;
 r[u256 (64 * off + 16) * 2 / 32] = ymm5;
 r[u256 (64 * off + 32) * 2 / 32] = ymm6;
 r[u256 (64 * off + 48) * 2 / 32] = ymm7;
 r[u256 (64 * off + 128) * 2 / 32] = ymm8;
 r[u256 (64 * off + 144) * 2 / 32] = ymm9;
 r[u256 (64 * off + 160) * 2 / 32] = ymm10;
 r[u256 (64 * off + 176) * 2 / 32] = ymm11;

 return r;
}


fn poly_invntt_tomont_0(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0;

 ymm0 = PDATA0[u256 _16XP * 2 / 32];

 r = intt_levels0t6(r, a, PDATA0, 0, ymm0);
 r = intt_levels0t6(r, a, PDATA0, 1, ymm0);

 r = intt_level7(r, PDATA0, 0, ymm0);
 r = intt_level7(r, PDATA0, 1, ymm0);

 return r;
}



fn poly_invntt_tomont_1(reg ptr u16[SABER_N] r, reg ptr u16[SABER_N] a) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0;

 ymm0 = PDATA1[u256 _16XP * 2 / 32];

 r = intt_levels0t6(r, a, PDATA1, 0, ymm0);
 r = intt_levels0t6(r, a, PDATA1, 1, ymm0);

 r = intt_level7(r, PDATA1, 0, ymm0);
 r = intt_level7(r, PDATA1, 1, ymm0);

 return r;
}

inline fn polyvec_invntt_tomont_0(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_invntt_tomont_0(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N]);
 }

 return r;
}

inline fn polyvec_invntt_tomont_1(reg ptr u16[SABER_KN] r, reg ptr u16[SABER_KN] a) -> reg ptr u16[SABER_KN]
{
 inline int i;

 for i = 0 to SABER_K {
  r[i * SABER_N:SABER_N] = poly_invntt_tomont_1(r[i * SABER_N:SABER_N], a[i * SABER_N:SABER_N]);
 }

 return r;
}
inline fn pointwise32(reg ptr u16[SABER_N] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b, reg ptr u16[864] pdata, inline int off, reg u256 ymm0, reg u256 ymm1) -> reg ptr u16[SABER_N]
{
 reg u256 ymm3, ymm4, ymm5, ymm6, ymm7, ymm8, ymm9, ymm10, ymm11, ymm12, ymm13, ymm14, ymm15;

 ymm4 = a[u256 (0 + 32 * off + 0) * 2 / 32];
 ymm5 = b[u256 (0 + 32 * off + 0) * 2 / 32];

 ymm6 = a[u256 (0 + 32 * off + 16) * 2 / 32];
 ymm7 = b[u256 (0 + 32 * off + 16) * 2 / 32];

 ymm8 = a[u256 (256 + 32 * off + 0) * 2 / 32];
 ymm9 = b[u256 (256 + 32 * off + 0) * 2 / 32];

 ymm3 = #VPMULL_16u16(ymm4, ymm5);
 ymm4 = #VPMULH_16u16(ymm4, ymm5);

 ymm10 = a[u256 (256 + 32 * off + 16) * 2 / 32];
 ymm11 = b[u256 (256 + 32 * off + 16) * 2 / 32];

 ymm5 = #VPMULL_16u16(ymm6, ymm7);
 ymm6 = #VPMULH_16u16(ymm6, ymm7);

 ymm12 = a[u256 (512 + 32 * off + 0) * 2 / 32];
 ymm13 = b[u256 (512 + 32 * off + 0) * 2 / 32];

 ymm7 = #VPMULL_16u16(ymm8, ymm9);
 ymm8 = #VPMULH_16u16(ymm8, ymm9);

 ymm14 = a[u256 (512 + 32 * off + 16) * 2 / 32];
 ymm15 = b[u256 (512 + 32 * off + 16) * 2 / 32];

 ymm9 = #VPMULL_16u16(ymm10, ymm11);
 ymm10 = #VPMULH_16u16(ymm10, ymm11);

 ymm11 = #VPMULL_16u16(ymm12, ymm13);
 ymm12 = #VPMULH_16u16(ymm12, ymm13);

 ymm13 = #VPMULL_16u16(ymm14, ymm15);
 ymm14 = #VPMULH_16u16(ymm14, ymm15);

 ymm3 = #VPMULL_16u16(ymm3, ymm1);
 ymm5 = #VPMULL_16u16(ymm5, ymm1);
 ymm7 = #VPMULL_16u16(ymm7, ymm1);
 ymm9 = #VPMULL_16u16(ymm9, ymm1);
 ymm11 = #VPMULL_16u16(ymm11, ymm1);
 ymm13 = #VPMULL_16u16(ymm13, ymm1);

 ymm3 = #VPMULH_16u16(ymm3, ymm0);
 ymm5 = #VPMULH_16u16(ymm5, ymm0);
 ymm7 = #VPMULH_16u16(ymm7, ymm0);
 ymm9 = #VPMULH_16u16(ymm9, ymm0);
 ymm11 = #VPMULH_16u16(ymm11, ymm0);
 ymm13 = #VPMULH_16u16(ymm13, ymm0);

 ymm4 +16u16= ymm8;
 ymm6 +16u16= ymm10;
 ymm4 +16u16= ymm12;
 ymm6 +16u16= ymm14;

 ymm3 = ymm4 -16u16 ymm3;
 ymm4 = ymm6 -16u16 ymm5;
 ymm3 = ymm3 -16u16 ymm7;
 ymm4 = ymm4 -16u16 ymm9;
 ymm3 = ymm3 -16u16 ymm11;
 ymm4 = ymm4 -16u16 ymm13;







 ymm14 = pdata[u256 _16XF_PINV * 2 / 32];
 ymm15 = pdata[u256 _16XF * 2 / 32];


 ymm3, ymm5 = fqmulprecomp_0(ymm14, ymm15, ymm3, ymm5, ymm0);


 ymm4, ymm5 = fqmulprecomp_0(ymm14, ymm15, ymm4, ymm5, ymm0);

 r[u256 (32 * off + 0) * 2 / 32] = ymm3;
 r[u256 (32 * off + 16) * 2 / 32] = ymm4;

 return r;
}

fn polyvec_basemul_acc_montgomery_0(reg ptr u16[SABER_N] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0, ymm1, ymm2;

 ymm0 = PDATA0[u256 _16XP * 2 / 32];
 ymm1 = PDATA0[u256 _16XPINV * 2 / 32];

 r = pointwise32(r, a, b, PDATA0, 0, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 1, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 2, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 3, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 4, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 5, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 6, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA0, 7, ymm0, ymm1);

 return r;
}

fn polyvec_basemul_acc_montgomery_1(reg ptr u16[SABER_N] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b) -> reg ptr u16[SABER_N]
{
 reg u256 ymm0, ymm1, ymm2;

 ymm0 = PDATA1[u256 _16XP * 2 / 32];
 ymm1 = PDATA1[u256 _16XPINV * 2 / 32];

 r = pointwise32(r, a, b, PDATA1, 0, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 1, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 2, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 3, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 4, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 5, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 6, ymm0, ymm1);
 r = pointwise32(r, a, b, PDATA1, 7, ymm0, ymm1);

 return r;
}

fn polyvec_matrix_vector_mul(reg ptr u16[SABER_KN] t, reg ptr u16[SABER_KKN] a, reg ptr u16[SABER_KN] s, reg u64 transpose) -> reg ptr u16[SABER_KN]
{
 inline int i;
 inline int j;

 stack u16[SABER_KN] ahat;
 stack u16[SABER_KN] shat;
 stack u16[SABER_KN] t0, t1;
 stack u16[SABER_KN] t00, t11;

 shat = polyvec_ntt_0(shat, s);
 for i = 0 to SABER_K {
  for j = 0 to SABER_K {
   if (transpose != 0) {
    ahat[j * SABER_N:SABER_N] = poly_ntt_0(ahat[j * SABER_N:SABER_N], a[j * SABER_KN + i * SABER_N:SABER_N]);
   } else {
    ahat[j * SABER_N:SABER_N] = poly_ntt_0(ahat[j * SABER_N:SABER_N], a[i * SABER_KN + j * SABER_N:SABER_N]);
   }
  }
  t00[i * SABER_N:SABER_N] = polyvec_basemul_acc_montgomery_0(t00[i * SABER_N:SABER_N], ahat, shat);
 }

 shat = polyvec_ntt_1(shat, s);
 for i = 0 to SABER_K {
  for j = 0 to SABER_K {
   if (transpose != 0) {
    ahat[j * SABER_N:SABER_N] = poly_ntt_1(ahat[j * SABER_N:SABER_N], a[j * SABER_KN + i * SABER_N:SABER_N]);
   } else {
    ahat[j * SABER_N:SABER_N] = poly_ntt_1(ahat[j * SABER_N:SABER_N], a[i * SABER_KN + j * SABER_N:SABER_N]);
   }
  }
  t11[i * SABER_N:SABER_N] = polyvec_basemul_acc_montgomery_1(t11[i * SABER_N:SABER_N], ahat, shat);
 }

 t0 = polyvec_invntt_tomont_0(t0, t00);
 t1 = polyvec_invntt_tomont_1(t1, t11);

 t = polyvec_crt(t, t0, t1);

 return t;
}
fn polyvec_iprod(reg ptr u16[SABER_N] r, reg ptr u16[SABER_KN] a, reg ptr u16[SABER_KN] b) -> reg ptr u16[SABER_N]
{
 stack u16[SABER_N] t0, t1;
 stack u16[SABER_N] r0, r1;
 stack u16[SABER_KN] ahat;
 stack u16[SABER_KN] bhat;

 ahat = polyvec_ntt_0(ahat, a);
 bhat = polyvec_ntt_0(bhat, b);
 t0 = polyvec_basemul_acc_montgomery_0(t0, ahat, bhat);

 bhat = polyvec_ntt_1(bhat, b);
 ahat = polyvec_ntt_1(ahat, a);
 t1 = polyvec_basemul_acc_montgomery_1(t1, ahat, bhat);

 r0 = poly_invntt_tomont_0(r0, t0);
 r1 = poly_invntt_tomont_1(r1, t1);

 r = poly_crt(r, r0, r1);

 return r;
}

export fn polyvec_crt_jazz(reg u64 rp, reg u64 ap, reg u64 bp)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KN] a;
 stack u16[SABER_KN] b;
 stack u16[SABER_KN] r;

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [bp + 32 * i];
  b[u256 i] = t256;
 }

 r = polyvec_crt(r, a, b);

 for i = 0 to SABER_KN / 16 {
  t256 = r[u256 i];
  (u256) [rp + 32 * i] = t256;
 }
}

export fn polyvec_ntt_0_jazz(reg u64 rp, reg u64 ap)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KN] a;
 stack u16[SABER_KN] r;

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 r = polyvec_ntt_0(r, a);

 for i = 0 to SABER_KN / 16 {
  t256 = r[u256 i];
  (u256) [rp + 32 * i] = t256;
 }
}

export fn polyvec_ntt_1_jazz(reg u64 rp, reg u64 ap)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KN] a;
 stack u16[SABER_KN] r;

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 r = polyvec_ntt_1(r, a);

 for i = 0 to SABER_KN / 16 {
  t256 = r[u256 i];
  (u256) [rp + 32 * i] = t256;
 }
}

export fn polyvec_invntt_tomont_0_jazz(reg u64 rp, reg u64 ap)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KN] a;
 stack u16[SABER_KN] r;

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 r = polyvec_invntt_tomont_0(r, a);

 for i = 0 to SABER_KN / 16 {
  t256 = r[u256 i];
  (u256) [rp + 32 * i] = t256;
 }
}

export fn polyvec_invntt_tomont_1_jazz(reg u64 rp, reg u64 ap)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KN] a;
 stack u16[SABER_KN] r;

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 r = polyvec_invntt_tomont_1(r, a);

 for i = 0 to SABER_KN / 16 {
  t256 = r[u256 i];
  (u256) [rp + 32 * i] = t256;
 }
}


export fn polyvec_basemul_acc_montgomery_0_jazz(reg u64 rp, reg u64 ap, reg u64 bp)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KN] a;
 stack u16[SABER_KN] b;
 stack u16[SABER_N] r;

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [bp + 32 * i];
  b[u256 i] = t256;
 }

 r = polyvec_basemul_acc_montgomery_0(r, a, b);

 for i = 0 to SABER_N / 16 {
  t256 = r[u256 i];
  (u256) [rp + 32 * i] = t256;
 }
}

export fn polyvec_basemul_acc_montgomery_1_jazz(reg u64 rp, reg u64 ap, reg u64 bp)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KN] a;
 stack u16[SABER_KN] b;
 stack u16[SABER_N] r;

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [bp + 32 * i];
  b[u256 i] = t256;
 }

 r = polyvec_basemul_acc_montgomery_1(r, a, b);

 for i = 0 to SABER_N / 16 {
  t256 = r[u256 i];
  (u256) [rp + 32 * i] = t256;
 }
}


export fn polyvec_matrix_vector_mul_jazz(reg u64 tp, reg u64 ap, reg u64 sp, reg u64 transpose)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KKN] a;
 stack u16[SABER_KN] s;
 stack u16[SABER_KN] t;

 for i = 0 to SABER_KKN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [sp + 32 * i];
  s[u256 i] = t256;
 }

 t = polyvec_matrix_vector_mul(t, a, s, transpose);

 for i = 0 to SABER_KN / 16 {
  t256 = t[u256 i];
  (u256) [tp + 32 * i] = t256;
 }
}


export fn polyvec_iprod_jazz(reg u64 rp, reg u64 ap, reg u64 bp)
{
 inline int i;

 reg u256 t256;

 stack u16[SABER_KN] a;
 stack u16[SABER_KN] b;
 stack u16[SABER_N] r;

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [ap + 32 * i];
  a[u256 i] = t256;
 }

 for i = 0 to SABER_KN / 16 {
  t256 = (u256) [bp + 32 * i];
  b[u256 i] = t256;
 }

 r = polyvec_iprod(r, a, b);

 for i = 0 to SABER_N / 16 {
  t256 = r[u256 i];
  (u256) [rp + 32 * i] = t256;
 }
}
